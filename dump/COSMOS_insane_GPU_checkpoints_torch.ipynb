{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3303812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded zlibwapi.dll\n",
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_22596\\344887725.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 1: Imports, Mixed Precision, GPU Setup, Configuration (PyTorch Version)\n",
    "\n",
    "import os, glob, ctypes, math\n",
    "import h5py, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, accuracy_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Mixed precision utilities\n",
    "scaler = GradScaler()\n",
    "\n",
    "# DLL fix for Windows HDF5/zlib\n",
    "try:\n",
    "    dll = os.path.join(os.environ.get('CONDA_PREFIX',''), 'Library', 'bin', 'zlibwapi.dll')\n",
    "    ctypes.CDLL(dll)\n",
    "    print(\"Loaded zlibwapi.dll\")\n",
    "except Exception:\n",
    "    print(\"Could not load zlibwapi.dll\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e45fa5",
   "metadata": {},
   "source": [
    "## Data‑Loading & Generator Functions (unchanged except for PyTorch tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c52bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Data Loader & Dataset\n",
    "\n",
    "SEQ_LEN     = 4       # sequence length\n",
    "PATCH_SIZE  = 256     # size of random crop\n",
    "BATCH_SIZE  = 4\n",
    "EPOCHS      = 10\n",
    "\n",
    "def load_multi(fp_seq):\n",
    "    frames = []\n",
    "    for fp in fp_seq[:SEQ_LEN]:\n",
    "        with h5py.File(fp,'r') as f:\n",
    "            cnt1, cnt2   = f['IMG_TIR1'][0][...], f['IMG_TIR2'][0][...]\n",
    "            cnt_wv, cnt_mir = f['IMG_WV'][0][...], f['IMG_MIR'][0][...]\n",
    "            cnt_vis      = f['IMG_VIS'][0][...]\n",
    "            lut1, lut2   = f['IMG_TIR1_TEMP'][:],  f['IMG_TIR2_TEMP'][:]\n",
    "            lut_wv, lut_mir = f['IMG_WV_TEMP'][:], f['IMG_MIR_TEMP'][:]\n",
    "            lut_vis      = f['IMG_VIS_ALBEDO'][:]\n",
    "        bt1 = lut1[cnt1]; bt2 = lut2[cnt2]\n",
    "        wv  = lut_wv[cnt_wv]; mir = lut_mir[cnt_mir]\n",
    "        vis = lut_vis[cnt_vis]\n",
    "        frames.append(np.stack([bt1,bt2,wv,mir,vis],axis=-1)/300.0)\n",
    "    X = np.stack(frames,axis=0).astype(np.float32)   # (T,H,W,C)\n",
    "\n",
    "    with h5py.File(fp_seq[-1],'r') as f:\n",
    "        cnt1, cnt2   = f['IMG_TIR1'][0][...], f['IMG_TIR2'][0][...]\n",
    "        cnt_wv, cnt_mir = f['IMG_WV'][0][...], f['IMG_MIR'][0][...]\n",
    "        lut1, lut2   = f['IMG_TIR1_TEMP'][:],  f['IMG_TIR2_TEMP'][:]\n",
    "        lut_wv, lut_mir = f['IMG_WV_TEMP'][:], f['IMG_MIR_TEMP'][:]\n",
    "    bt1_t = lut1[cnt1]; bt2_t = lut2[cnt2]\n",
    "    wv_t  = lut_wv[cnt_wv]; mir_t = lut_mir[cnt_mir]\n",
    "\n",
    "    # Example label generation identical to original\n",
    "    cloud_mask  = (bt1 < 240).astype(np.float32)[None,...]   # dummy rule\n",
    "    conv_mask   = (bt2 < 235).astype(np.float32)[None,...]\n",
    "    fog_mask    = (wv  < 240).astype(np.float32)[None,...]\n",
    "    moisture    = wv_t[...,None]/300.0\n",
    "    thermo_contrast = (bt1_t-bt2_t)[...,None]/50.0\n",
    "    temp_trend  = np.array([(bt1_t.mean()-bt1.mean())/10.0], dtype=np.float32)\n",
    "\n",
    "    y = {\n",
    "        'cloud'          : cloud_mask,\n",
    "        'convective'     : conv_mask,\n",
    "        'fog'            : fog_mask,\n",
    "        'moisture'       : moisture,\n",
    "        'thermo_contrast': thermo_contrast,\n",
    "        'temp_trend'     : temp_trend\n",
    "    }\n",
    "    return X, y\n",
    "\n",
    "def random_crop(X,y):\n",
    "    H,W = X.shape[1], X.shape[2]\n",
    "    i = np.random.randint(0, H-PATCH_SIZE)\n",
    "    j = np.random.randint(0, W-PATCH_SIZE)\n",
    "    Xc = X[:, i:i+PATCH_SIZE, j:j+PATCH_SIZE, :]\n",
    "    yc = {}\n",
    "    for k,v in y.items():\n",
    "        if v.ndim==3:  # spatial map\n",
    "            yc[k] = v[:, i:i+PATCH_SIZE, j:j+PATCH_SIZE]\n",
    "        else:\n",
    "            yc[k] = v\n",
    "    return Xc, yc\n",
    "\n",
    "class INSATSequenceDataset(Dataset):\n",
    "    def __init__(self, seqs, transforms=True):\n",
    "        self.seqs = seqs\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, idx):\n",
    "        X,y = load_multi(self.seqs[idx])\n",
    "        if self.transforms:\n",
    "            X,y = random_crop(X,y)\n",
    "        # (T,H,W,C) -> (C,T,H,W) for ConvLSTM\n",
    "        X = torch.from_numpy(X).permute(3,0,1,2)   # (C,T,H,W)\n",
    "        y_tensors={}\n",
    "        for k,v in y.items():\n",
    "            y_tensors[k] = torch.from_numpy(v)\n",
    "        return X, y_tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b1d10",
   "metadata": {},
   "source": [
    "## Dataset Split & DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85f4f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define sequences (example: list of file paths or data sequences)\n",
    "sequences = [f\"data/file_{i}.h5\" for i in range(100)]  # Replace with actual file paths or data\n",
    "\n",
    "split       = int(0.9*len(sequences))\n",
    "train_seqs  = sequences[:split]\n",
    "val_seqs    = sequences[split:]\n",
    "\n",
    "train_ds = INSATSequenceDataset(train_seqs, transforms=True)\n",
    "val_ds   = INSATSequenceDataset(val_seqs, transforms=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01aacd1",
   "metadata": {},
   "source": [
    "## Model Definition (ConvLSTM + Multi‑Head Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0849d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastMultitaskNowcast(\n",
      "  (convlstm): ConvLSTM(\n",
      "    (cells): ModuleList(\n",
      "      (0): ConvLSTMCell(\n",
      "        (Gates): Conv2d(37, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): ConvLSTMCell(\n",
      "        (Gates): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (cloud): Sequential(\n",
      "      (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (convective): Sequential(\n",
      "      (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (fog): Sequential(\n",
      "      (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (moisture): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (thermo_contrast): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (temp_head): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Model Definition & Compilation (PyTorch)\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size//2\n",
    "        self.input_channels  = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.Gates = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                               out_channels=4 * hidden_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               padding=padding,\n",
    "                               bias=bias)\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        gates = self.Gates(combined)\n",
    "        i,f,o,g = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i); f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o); g = torch.tanh(g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, spatial_size, device):\n",
    "        height, width = spatial_size\n",
    "        h = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)\n",
    "        c = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)\n",
    "        return (h,c)\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels if isinstance(hidden_channels,list) else [hidden_channels]*num_layers\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size,list) else [kernel_size]*num_layers\n",
    "\n",
    "        cells = []\n",
    "        chs_in = input_channels\n",
    "        for i in range(num_layers):\n",
    "            cells.append(ConvLSTMCell(\n",
    "                input_channels=chs_in,\n",
    "                hidden_channels=self.hidden_channels[i],\n",
    "                kernel_size=self.kernel_size[i],\n",
    "                bias=bias\n",
    "            ))\n",
    "            chs_in = self.hidden_channels[i]\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,T,H,W)\n",
    "        B,C,T,H,W = x.shape\n",
    "        h,c = [],[]\n",
    "        for i in range(self.num_layers):\n",
    "            h.append(None); c.append(None)\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            inp = x[:,:,t]\n",
    "            for i,cell in enumerate(self.cells):\n",
    "                if h[i] is None:\n",
    "                    h[i], c[i] = cell.init_hidden(B, (H,W), x.device)\n",
    "                h[i], c[i] = cell(inp, (h[i],c[i]))\n",
    "                inp = h[i]\n",
    "            outputs.append(h[-1])\n",
    "        # return last hidden state\n",
    "        return outputs[-1]\n",
    "\n",
    "class FastMultitaskNowcast(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convlstm = ConvLSTM(input_channels=5, hidden_channels=[32,16], kernel_size=3, num_layers=2)\n",
    "        # heads\n",
    "        self.heads = nn.ModuleDict({\n",
    "            'cloud': nn.Sequential(nn.Conv2d(16,1,1), nn.Sigmoid()),\n",
    "            'convective': nn.Sequential(nn.Conv2d(16,1,1), nn.Sigmoid()),\n",
    "            'fog': nn.Sequential(nn.Conv2d(16,1,1), nn.Sigmoid()),\n",
    "            'moisture': nn.Conv2d(16,1,1),\n",
    "            'thermo_contrast': nn.Conv2d(16,1,1),\n",
    "        })\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.temp_head = nn.Linear(16,1)\n",
    "    def forward(self, x):\n",
    "        # x: (B,C,T,H,W)\n",
    "        feat = self.convlstm(x)   # (B,16,H,W)\n",
    "        out = {\n",
    "            k: head(feat) for k,head in self.heads.items()\n",
    "        }\n",
    "        pooled = self.pool(feat).flatten(1)\n",
    "        out['temp_trend'] = self.temp_head(pooled)\n",
    "        return out\n",
    "\n",
    "model = FastMultitaskNowcast().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a98e87",
   "metadata": {},
   "source": [
    "## Training with Progress Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41500c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/23 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 25348, 8436, 20972, 1952) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Xb, yb \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     22\u001b[0m     Xb \u001b[38;5;241m=\u001b[39m Xb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m     yb \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m yb\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 25348, 8436, 20972, 1952) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 5: Training Loop\n",
    "\n",
    "loss_fns = {\n",
    "    'cloud': nn.BCELoss(),\n",
    "    'convective': nn.BCELoss(),\n",
    "    'fog': nn.BCELoss(),\n",
    "    'moisture': nn.MSELoss(),\n",
    "    'thermo_contrast': nn.MSELoss(),\n",
    "    'temp_trend': nn.MSELoss()\n",
    "}\n",
    "loss_weights = {'cloud':1,'convective':1,'fog':1,'moisture':0.5,'thermo_contrast':0.5,'temp_trend':0.1}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "os.makedirs(\"checkpoints_3\", exist_ok=True)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    running_loss = 0.0\n",
    "    for Xb, yb in pbar:\n",
    "        Xb = Xb.to(device)\n",
    "        yb = {k:v.to(device) for k,v in yb.items()}\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            preds = model(Xb)\n",
    "            losses = []\n",
    "            for k in preds:\n",
    "                pred = preds[k]\n",
    "                target = yb[k]\n",
    "                if pred.ndim==4 and target.ndim==4:\n",
    "                    pred = pred.squeeze(1)\n",
    "                    target = target.squeeze(1)\n",
    "                loss = loss_fns[k](pred.float(), target.float())\n",
    "                losses.append(loss * loss_weights[k])\n",
    "            loss_total = sum(losses)\n",
    "        scaler.scale(loss_total).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss_total.item()\n",
    "        pbar.set_postfix(loss=running_loss/ (pbar.n+1))\n",
    "    # ── validation ----------------------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = {k:v.to(device) for k,v in yb.items()}\n",
    "            with autocast():\n",
    "                preds = model(Xb)\n",
    "                losses = []\n",
    "                for k in preds:\n",
    "                    pred = preds[k]\n",
    "                    target = yb[k]\n",
    "                    if pred.ndim==4 and target.ndim==4:\n",
    "                        pred = pred.squeeze(1)\n",
    "                        target = target.squeeze(1)\n",
    "                    loss = loss_fns[k](pred.float(), target.float())\n",
    "                    losses.append(loss * loss_weights[k])\n",
    "                val_loss += sum(losses).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), f\"checkpoints_3/model_epoch_{epoch:02d}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78991986",
   "metadata": {},
   "source": [
    "## Evaluation & Metrics Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6 — Evaluation\n",
    "seg_keys = [\"cloud\",\"convective\",\"fog\"]\n",
    "reg_keys = [\"moisture\",\"thermo_contrast\",\"temp_trend\"]\n",
    "\n",
    "y_true_seg, y_pred_seg = {k:[] for k in seg_keys}, {k:[] for k in seg_keys}\n",
    "y_true_reg, y_pred_reg = {k:[] for k in reg_keys}, {k:[] for k in reg_keys}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in tqdm(val_loader, desc=\"Evaluation\"):\n",
    "        Xb = Xb.to(device)\n",
    "        preds = model(Xb)\n",
    "        for k in seg_keys:\n",
    "            truth = yb[k].view(-1).numpy()\n",
    "            pred  = (preds[k].cpu().view(-1) > 0.5).numpy()\n",
    "            y_true_seg[k].append(truth)\n",
    "            y_pred_seg[k].append(pred)\n",
    "        for k in reg_keys:\n",
    "            truth = yb[k].view(-1).numpy()\n",
    "            pred  = preds[k].cpu().view(-1).numpy()\n",
    "            y_true_reg[k].append(truth)\n",
    "            y_pred_reg[k].append(pred)\n",
    "\n",
    "for d in (y_true_seg, y_pred_seg, y_true_reg, y_pred_reg):\n",
    "    for k in d: d[k] = np.concatenate(d[k])\n",
    "\n",
    "# ── segmentation --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "seg_rows=[]\n",
    "for k in seg_keys:\n",
    "    cm = confusion_matrix(y_true_seg[k], y_pred_seg[k])\n",
    "    seg_rows.append({\n",
    "        \"Task\":k,\n",
    "        \"Accuracy\": accuracy_score(y_true_seg[k], y_pred_seg[k]),\n",
    "        \"Precision\": precision_score(y_true_seg[k], y_pred_seg[k]),\n",
    "        \"Recall\": recall_score(y_true_seg[k], y_pred_seg[k]),\n",
    "        \"F1\": f1_score(y_true_seg[k], y_pred_seg[k])\n",
    "    })\n",
    "df_seg = pd.DataFrame(seg_rows).set_index(\"Task\")\n",
    "print(\"### Segmentation\"); display(df_seg)\n",
    "\n",
    "# ── regression ---------------------------------------------------------------\n",
    "reg_rows=[]\n",
    "for k in reg_keys:\n",
    "    reg_rows.append({\n",
    "        \"Task\":k,\n",
    "        \"MSE\": mean_squared_error(y_true_reg[k], y_pred_reg[k]),\n",
    "        \"MAE\": mean_absolute_error(y_true_reg[k], y_pred_reg[k])\n",
    "    })\n",
    "df_reg = pd.DataFrame(reg_rows).set_index(\"Task\")\n",
    "print(\"### Regression\"); display(df_reg)\n",
    "\n",
    "# ── confusion matrices --------------------------------------------------------\n",
    "for k in seg_keys:\n",
    "    cm = confusion_matrix(y_true_seg[k], y_pred_seg[k])\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title(f\"{k.capitalize()} – Confusion matrix\")\n",
    "    plt.imshow(cm, cmap=\"Blues\"); plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "    for (i,j),v in np.ndenumerate(cm): plt.text(j,i,str(v), ha=\"center\", va=\"center\")\n",
    "    plt.colorbar(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
