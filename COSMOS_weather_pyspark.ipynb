{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COSMOS Weather Nowcasting: PySpark Data Prep & Keras Training\n",
        "\n",
        "This notebook demonstrates a workflow combining PySpark for data preparation and Keras/TensorFlow for training a ConvLSTM model for weather nowcasting based on INSAT-3DR data.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Phase 1 (PySpark):** Read raw HDF5 files, process time sequences, apply normalization and cropping, and save the prepared data to Parquet format (using chunking to manage memory).\n",
        "2.  **Phase 2 (Keras):** Load the processed data from Parquet, define the ConvLSTM model, train the model (with checkpointing/resuming), and evaluate its performance with detailed metrics and visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Imports, Setup, and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dc6fdc3b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Cell 1: Initializing ---\n",
            "PySpark libraries imported successfully.\n",
            "\n",
            "--- Hardware & Library Setup ---\n",
            "Info: Mixed precision explicitly disabled.\n",
            "Using 1 GPU(s) with memory growth enabled.\n",
            "Using TensorFlow version: 2.8.0\n",
            "Using NumPy version: 1.21.6\n",
            "\n",
            "--- Environment Variable Checks ---\n",
            "PYSPARK_PYTHON set to: c:\\Users\\dhanu\\.conda\\envs\\w\\python.exe\n",
            "HADOOP_HOME environment variable found: C:\\hadoop\n",
            "winutils.exe found at: C:\\hadoop\\bin\\winutils.exe\n",
            "\n",
            "--- Cell 1: Setup Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 1: Imports, Setup, and Configuration\n",
        "# ==============================================================\n",
        "print(\"--- Cell 1: Initializing ---\")\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, precision_score, recall_score,\n",
        "    f1_score, accuracy_score, mean_squared_error, mean_absolute_error\n",
        ")\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
        "# import ctypes # No longer needed for DLL fix\n",
        "import re # For checkpoint resuming\n",
        "import gc # For garbage collection (optional memory management)\n",
        "import sys # To get python executable path\n",
        "from IPython.display import display # For better dataframe display\n",
        "import math # For chunk calculation\n",
        "import shutil # For removing old parquet dir\n",
        "\n",
        "# --- PySpark Imports (Needed for Phase 1) ---\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql.types import (\n",
        "        StructType, StructField, StringType, ArrayType,\n",
        "        FloatType, DoubleType\n",
        "    )\n",
        "    pyspark_available = True\n",
        "    print(\"PySpark libraries imported successfully.\")\n",
        "except ImportError:\n",
        "    pyspark_available = False\n",
        "    print(\"\\033[91mWarning: PySpark libraries not found. Phase 1 will be skipped.\\033[0m\")\n",
        "    print(\"\\033[91mEnsure PySpark is installed in your environment (e.g., spark301_py39).\\033[0m\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# Data Paths\n",
        "# !!! IMPORTANT: Update these paths to match your system !!!\n",
        "RAW_DATA_DIR           = r\"C:\\college\\CV\\COSMOS\\6C_full\" # <--- POINT TO YOUR *FULL* HDF5 DATASET\n",
        "PROCESSED_PARQUET_DIR  = r\"C:\\college\\CV\\COSMOS\\processed_cosmos_data_pyspark.parquet\" # <--- Location for Spark output\n",
        "MODEL_SAVE_PATH        = r\"C:\\college\\CV\\COSMOS\\multitask_nowcast_pyspark_trained.h5\" # <--- Save path for Keras model\n",
        "CHECKPOINT_DIR         = \"checkpoints_pyspark_trained\" # <--- Checkpoint dir for Keras training\n",
        "\n",
        "# Model/Data Parameters\n",
        "SEQ_LEN       = 4\n",
        "PATCH_SIZE    = 32\n",
        "BATCH_SIZE    = 16        # Keras training batch size\n",
        "EPOCHS        = 20        # Keras training epochs\n",
        "THRESHOLD     = 265.0     # Cloud detection threshold\n",
        "CV_THRESHOLD  = 260.0     # Convection detection threshold\n",
        "FOG_THRESHOLD = 270.0     # Fog detection threshold\n",
        "\n",
        "# --- Hardware & Library Setup ---\n",
        "print(\"\\n--- Hardware & Library Setup ---\")\n",
        "# Mixed Precision is disabled\n",
        "print(\"Info: Mixed precision explicitly disabled.\")\n",
        "\n",
        "# DLL Fix Block Removed (assuming `conda install zlib` handled it)\n",
        "\n",
        "try: # GPU Setup\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        # Attempt to enable memory growth for GPUs to avoid allocating all memory at once\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Using {len(gpus)} GPU(s) with memory growth enabled.\")\n",
        "    else:\n",
        "        print(\"Info: No GPU detected by TensorFlow. Using CPU.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Error during GPU setup: {e}\")\n",
        "\n",
        "# Print Key Versions\n",
        "print(f\"Using TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Using NumPy version: {np.__version__}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Environment Variable Checks ---\")\n",
        "# Set PYSPARK_PYTHON (helps Spark find the right Python for workers)\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "print(f\"PYSPARK_PYTHON set to: {sys.executable}\")\n",
        "\n",
        "# Check for HADOOP_HOME (Crucial for Windows file operations)\n",
        "hadoop_home = os.environ.get('HADOOP_HOME')\n",
        "if hadoop_home:\n",
        "    print(f\"HADOOP_HOME environment variable found: {hadoop_home}\")\n",
        "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
        "    if os.path.exists(winutils_path):\n",
        "        print(f\"winutils.exe found at: {winutils_path}\")\n",
        "    else:\n",
        "        print(f\"\\033[91mWarning: winutils.exe not found at expected location: {winutils_path}\\033[0m\") # Red Warning\n",
        "        print(\"\\033[91mSpark file write operations might fail.\\033[0m\")\n",
        "else:\n",
        "    print(\"\\033[91mWarning: HADOOP_HOME environment variable is not set.\\033[0m\") # Red Warning\n",
        "    print(\"\\033[91mSpark file write operations will likely fail on Windows.\\033[0m\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Cell 1: Setup Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc05fd4",
      "metadata": {},
      "source": [
        "## Phase 1: PySpark Data Preparation\n",
        "\n",
        "This phase reads the raw HDF5 files, processes them into sequences, performs normalization and cropping, and saves the result as Parquet files using chunking to manage memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f759b87",
      "metadata": {},
      "source": [
        "### Cell 2: Define HDF5 Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8eda0a5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cell 2: Defining HDF5 Processing Function ---\n",
            "Function 'load_and_crop_sequence' defined.\n",
            "\n",
            "--- Cell 2: Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 2: Define HDF5 Processing Function\n",
        "# ==============================================================\n",
        "print(\"\\n--- Cell 2: Defining HDF5 Processing Function ---\")\n",
        "\n",
        "# Import necessary libraries for the function (already imported in Cell 1 but good practice)\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Make sure config variables from Cell 1 are available\n",
        "if 'SEQ_LEN' not in locals(): SEQ_LEN = 4 # Default if running independently\n",
        "if 'PATCH_SIZE' not in locals(): PATCH_SIZE = 32\n",
        "if 'THRESHOLD' not in locals(): THRESHOLD = 265.0\n",
        "if 'CV_THRESHOLD' not in locals(): CV_THRESHOLD = 260.0\n",
        "if 'FOG_THRESHOLD' not in locals(): FOG_THRESHOLD = 270.0\n",
        "\n",
        "# --- Data Loading/Cropping Function Definition ---\n",
        "# This function processes one sequence of files\n",
        "def load_and_crop_sequence(fp_seq):\n",
        "    \"\"\"\n",
        "    Loads, processes, and crops data for one sequence.\n",
        "    Returns a dictionary or None on error/skip conditions.\n",
        "    \"\"\"\n",
        "    if len(fp_seq) != SEQ_LEN + 1:\n",
        "        # print(f\"Debug: Incorrect sequence length {len(fp_seq)}. Skipping.\")\n",
        "        return None\n",
        "    try:\n",
        "        frames = []\n",
        "        for fp in fp_seq[:SEQ_LEN]:\n",
        "            # Use 'try-with-resources' style for file handling\n",
        "            with h5py.File(fp, 'r') as f:\n",
        "                # Extract counts\n",
        "                cnt1, cnt2 = f['IMG_TIR1'][0][...], f['IMG_TIR2'][0][...]\n",
        "                cnt_wv, cnt_mir = f['IMG_WV'][0][...], f['IMG_MIR'][0][...]\n",
        "                cnt_vis = f['IMG_VIS'][0][...]\n",
        "                # Extract LUTs\n",
        "                lut1, lut2 = f['IMG_TIR1_TEMP'][:], f['IMG_TIR2_TEMP'][:] \n",
        "                lut_wv, lut_mir = f['IMG_WV_TEMP'][:], f['IMG_MIR_TEMP'][:] \n",
        "                lut_vis = f['IMG_VIS_ALBEDO'][:] \n",
        "            # Apply LUTs\n",
        "            bt1 = lut1[cnt1]; bt2 = lut2[cnt2]\n",
        "            wv = lut_wv[cnt_wv]; mir = lut_mir[cnt_mir]\n",
        "            vis = lut_vis[cnt_vis]\n",
        "            # Stack channels & normalize\n",
        "            frames.append(np.stack([bt1, bt2, wv, mir, vis], axis=-1) / 300.0)\n",
        "        X = np.stack(frames, axis=0).astype(np.float32)\n",
        "        \n",
        "        # Load target frame data\n",
        "        with h5py.File(fp_seq[-1], 'r') as f:\n",
        "            cnt1_t, cnt2_t = f['IMG_TIR1'][0][...], f['IMG_TIR2'][0][...]\n",
        "            cnt_wv_t, cnt_mir_t = f['IMG_WV'][0][...], f['IMG_MIR'][0][...]\n",
        "            lut1_t, lut2_t = f['IMG_TIR1_TEMP'][:], f['IMG_TIR2_TEMP'][:]\n",
        "            lut_wv_t, lut_mir_t = f['IMG_WV_TEMP'][:], f['IMG_MIR_TEMP'][:]\n",
        "        bt1_t = lut1_t[cnt1_t]; bt2_t = lut2_t[cnt2_t]\n",
        "        wv_t = lut_wv_t[cnt_wv_t]; mir_t = lut_mir_t[cnt_mir_t]\n",
        "        \n",
        "        # Calculate targets\n",
        "        last_mean = bt1_t.mean() / 300.0\n",
        "        first_mean = X[0, ..., 0].mean()\n",
        "        temp_trend = np.array([last_mean - first_mean], dtype=np.float32)\n",
        "        \n",
        "        y = {\n",
        "            'cloud': (bt1_t < THRESHOLD).astype(np.float32)[..., None],\n",
        "            'convective': (bt1_t < CV_THRESHOLD).astype(np.float32)[..., None],\n",
        "            'fog': (mir_t < FOG_THRESHOLD).astype(np.float32)[..., None],\n",
        "            'moisture': (wv_t / 300.0).astype(np.float32)[..., None],\n",
        "            'thermo_contrast': ((bt2_t - bt1_t) / 100.0).astype(np.float32)[..., None],\n",
        "            'temp_trend': temp_trend\n",
        "        }\n",
        "        \n",
        "        # Apply Random Crop\n",
        "        H, W = X.shape[1], X.shape[2]\n",
        "        if H < PATCH_SIZE or W < PATCH_SIZE: \n",
        "            # print(f\"Debug: Skipping sequence {os.path.basename(fp_seq[0])} due to small dimensions ({H}x{W})\")\n",
        "            return None # Skip if too small\n",
        "        i = np.random.randint(0, H - PATCH_SIZE + 1)\n",
        "        j = np.random.randint(0, W - PATCH_SIZE + 1)\n",
        "        Xc = X[:, i:i + PATCH_SIZE, j:j + PATCH_SIZE, :]\n",
        "        yc = {}\n",
        "        for k, v in y.items():\n",
        "            yc[k] = v[i:i + PATCH_SIZE, j:j + PATCH_SIZE] if v.ndim == 3 else v\n",
        "        \n",
        "        # Return processed data as dictionary with lists\n",
        "        # Keys here will become column names in Parquet\n",
        "        return {\n",
        "            'sequence_id': os.path.basename(fp_seq[0]),\n",
        "            'input_features': Xc.tolist(),\n",
        "            'target_cloud': yc['cloud'].tolist(),\n",
        "            'target_convective': yc['convective'].tolist(),\n",
        "            'target_fog': yc['fog'].tolist(),\n",
        "            'target_moisture': yc['moisture'].tolist(),\n",
        "            'target_thermo_contrast': yc['thermo_contrast'].tolist(),\n",
        "            'target_temp_trend': float(yc['temp_trend'][0])\n",
        "        }\n",
        "    # Catch specific errors if possible, otherwise generic Exception\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: File not found in sequence starting with {fp_seq[0]}. Skipping.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "         print(f\"Warning: Missing dataset key '{e}' in sequence starting with {fp_seq[0]}. Skipping.\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        # Log other errors without stopping the whole process if possible\n",
        "        print(f\"Warning: Error processing sequence starting with {fp_seq[0]}: {e}\")\n",
        "        # import traceback # Uncomment for full traceback during debug\n",
        "        # traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "print(\"Function 'load_and_crop_sequence' defined.\")\n",
        "print(\"\\n--- Cell 2: Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79658070",
      "metadata": {},
      "source": [
        "### Cell 3: Spark Session & Process/Write in Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8b13a59d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cell 3: Initializing Spark & Processing/Writing Data in Chunks ---\n",
            "Initializing Spark Session...\n",
            "Attempting to start Spark with driver memory: 4g\n",
            "Spark Session Initialized (Version: 3.0.1).\n",
            "Defining Spark DataFrame schema...\n",
            "\n",
            "Starting chunk processing: 896 sequences in ~30 chunks of size 30.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08f38b31fcf9413b9f207deeca91f11b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Chunks:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully processed and wrote 896 sequences to C:\\college\\CV\\COSMOS\\processed_cosmos_data_pyspark.parquet.\n",
            "Stopping Spark Session...\n",
            "Spark Session stopped.\n",
            "\n",
            "--- Cell 3: Spark Operations Complete (or Skipped) ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 3: Spark Session & Process/Write in Chunks\n",
        "# ==============================================================\n",
        "print(\"\\n--- Cell 3: Initializing Spark & Processing/Writing Data in Chunks ---\")\n",
        "\n",
        "# Ensure necessary imports/variables are available\n",
        "import os \n",
        "import gc\n",
        "import math\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "import glob # Needed if sequences list needs recreation\n",
        "\n",
        "# Check if PySpark is available and if the previous cell produced data/function\n",
        "if 'pyspark_available' not in locals() or not pyspark_available:\n",
        "    print(\"Skipping Spark operations as PySpark is not available.\")\n",
        "elif 'load_and_crop_sequence' not in locals():\n",
        "    print(\"Error: 'load_and_crop_sequence' function not defined. Please run Cell 2 first.\")\n",
        "    raise NameError(\"Missing 'load_and_crop_sequence' function\")\n",
        "elif 'sequences' not in locals() or not sequences:\n",
        "    # Attempt to recreate sequences if Cell 2 wasn't run but Cell 1 was\n",
        "    if 'RAW_DATA_DIR' in locals() and 'SEQ_LEN' in locals():\n",
        "         print(\"Recreating sequences list...\")\n",
        "         all_files = sorted(glob.glob(os.path.join(RAW_DATA_DIR, \"*.h5\")))\n",
        "         if not all_files: raise FileNotFoundError(f\"Error: No .h5 files found in {RAW_DATA_DIR}.\")\n",
        "         sequences = [all_files[i:i+SEQ_LEN+1] for i in range(len(all_files)-SEQ_LEN)]\n",
        "         print(f\"Generated {len(sequences)} sequences.\")\n",
        "    else:\n",
        "         print(\"Error: 'sequences' list not found or is empty. Please run Cell 1 & 2 first.\")\n",
        "         raise NameError(\"Missing 'sequences' list.\")\n",
        "else:\n",
        "    # --- Spark Session Initialization ---\n",
        "    print(\"Initializing Spark Session...\")\n",
        "    spark = None # Initialize spark variable\n",
        "    try:\n",
        "        # Import Spark types needed for schema definition\n",
        "        from pyspark.sql import SparkSession\n",
        "        from pyspark.sql.types import (\n",
        "             StructType, StructField, StringType, ArrayType, DoubleType\n",
        "        )\n",
        "        \n",
        "        # Use reduced memory (e.g., 2g or 4g based on system)\n",
        "        spark_memory = \"4g\" # Adjust this based on your system RAM\n",
        "        print(f\"Attempting to start Spark with driver memory: {spark_memory}\")\n",
        "        spark = SparkSession.builder \\\n",
        "            .appName(\"COSMOS_Chunk_Write_Notebook\") \\\n",
        "            .config(\"spark.driver.memory\", spark_memory) \\\n",
        "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "            .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
        "            .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
        "            .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
        "            .getOrCreate()\n",
        "        print(f\"Spark Session Initialized (Version: {spark.version}).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Spark Session: {e}\")\n",
        "        print(\"Please ensure Java and PySpark are correctly installed and configured.\")\n",
        "        spark = None\n",
        "        \n",
        "    if spark: # Proceed only if Spark session was created successfully\n",
        "        write_success = True # Assume success initially\n",
        "        processed_count = 0\n",
        "        \n",
        "        # --- Define Spark Schema (same as before) ---\n",
        "        print(\"Defining Spark DataFrame schema...\")\n",
        "        feature_array_type = ArrayType(ArrayType(ArrayType(ArrayType(DoubleType()))))\n",
        "        target_image_array_type = ArrayType(ArrayType(ArrayType(DoubleType())))\n",
        "        schema = StructType([\n",
        "            StructField(\"sequence_id\", StringType(), True),\n",
        "            StructField(\"input_features\", feature_array_type, True),\n",
        "            StructField(\"target_cloud\", target_image_array_type, True),\n",
        "            StructField(\"target_convective\", target_image_array_type, True),\n",
        "            StructField(\"target_fog\", target_image_array_type, True),\n",
        "            StructField(\"target_moisture\", target_image_array_type, True),\n",
        "            StructField(\"target_thermo_contrast\", target_image_array_type, True),\n",
        "            StructField(\"target_temp_trend\", DoubleType(), True)\n",
        "        ])\n",
        "        \n",
        "        # --- Process and Write in Chunks ---\n",
        "        chunk_size = 30 # Process 30 sequences at a time (adjust if needed based on RAM)\n",
        "        num_sequences = len(sequences)\n",
        "        num_chunks = math.ceil(num_sequences / chunk_size)\n",
        "        \n",
        "        # Ensure HADOOP_HOME check for Windows\n",
        "        if os.name == 'nt' and not os.environ.get('HADOOP_HOME'):\n",
        "             print(\"\\n\\033[91mError: HADOOP_HOME is not set. Cannot write Parquet on Windows.\\033[0m\")\n",
        "             write_success = False # Prevent loop from running\n",
        "        else:\n",
        "             print(f\"\\nStarting chunk processing: {num_sequences} sequences in ~{num_chunks} chunks of size {chunk_size}.\")\n",
        "             # Delete existing Parquet directory if it exists before starting append loop\n",
        "             if os.path.exists(PROCESSED_PARQUET_DIR):\n",
        "                  print(f\"Removing existing Parquet directory: {PROCESSED_PARQUET_DIR}\")\n",
        "                  try:\n",
        "                       shutil.rmtree(PROCESSED_PARQUET_DIR)\n",
        "                       print(\"Existing directory removed.\")\n",
        "                  except OSError as e:\n",
        "                       print(f\"\\033[91mError removing directory {PROCESSED_PARQUET_DIR}: {e}\\033[0m\")\n",
        "                       write_success = False # Abort if cleanup fails\n",
        "                       \n",
        "                       \n",
        "        # Loop through sequences in chunks\n",
        "        if write_success: # Proceed only if HADOOP_HOME is set and cleanup worked\n",
        "            for i in tqdm(range(num_chunks), desc=\"Processing Chunks\"):\n",
        "                start_idx = i * chunk_size\n",
        "                end_idx = min((i + 1) * chunk_size, num_sequences)\n",
        "                current_chunk_sequences = sequences[start_idx:end_idx]\n",
        "                \n",
        "                # print(f\"\\nProcessing chunk {i+1}/{num_chunks} (sequences {start_idx} to {end_idx-1})...\") # Less verbose\n",
        "                \n",
        "                # Process current chunk into a list\n",
        "                chunk_data_list = []\n",
        "                for seq in current_chunk_sequences: # Use simple loop for clarity\n",
        "                    processed_data = load_and_crop_sequence(seq)\n",
        "                    if processed_data:\n",
        "                        chunk_data_list.append(processed_data)\n",
        "                        \n",
        "                if not chunk_data_list:\n",
        "                    print(f\"Warning: No data processed in chunk {i+1}. Skipping write.\")\n",
        "                    continue # Move to the next chunk\n",
        "                    \n",
        "                # Create Spark DataFrame for the chunk\n",
        "                df_chunk = None # Initialize\n",
        "                try:\n",
        "                    # print(f\"Creating Spark DataFrame for chunk {i+1} ({len(chunk_data_list)} rows)...\") # Less verbose\n",
        "                    df_chunk = spark.createDataFrame(chunk_data_list, schema=schema)\n",
        "                    \n",
        "                    # Write chunk DataFrame using \"append\" mode\n",
        "                    # print(f\"Appending chunk {i+1} to Parquet: {PROCESSED_PARQUET_DIR}\") # Less verbose\n",
        "                    df_chunk.write.mode(\"append\").parquet(PROCESSED_PARQUET_DIR)\n",
        "                    processed_count += len(chunk_data_list)\n",
        "                    # print(f\"Chunk {i+1} written successfully.\") # Less verbose\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"\\033[91mError processing or writing chunk {i+1}: {e}\\033[0m\")\n",
        "                    write_success = False\n",
        "                    break # Stop processing if a chunk fails\n",
        "                finally:\n",
        "                     # Clear memory for this chunk\n",
        "                     del chunk_data_list\n",
        "                     if df_chunk is not None: del df_chunk\n",
        "                     gc.collect()\n",
        "                     \n",
        "        # --- Final Summary & Cleanup ---\n",
        "        if write_success and processed_count > 0:\n",
        "             print(f\"\\nSuccessfully processed and wrote {processed_count} sequences to {PROCESSED_PARQUET_DIR}.\")\n",
        "        elif not write_success:\n",
        "             print(\"\\n\\033[91mError occurred during chunk processing/writing. Parquet data might be incomplete or corrupted.\\033[0m\")\n",
        "        else:\n",
        "             print(\"\\nWarning: No data was successfully processed or written (check processing function and input files).\")\n",
        "             \n",
        "        print(\"Stopping Spark Session...\")\n",
        "        if spark: spark.stop()\n",
        "        print(\"Spark Session stopped.\")\n",
        "        \n",
        "        # Raise error if writing failed overall\n",
        "        if not write_success:\n",
        "             raise RuntimeError(\"Failed to write Parquet data completely due to errors in chunk processing.\")\n",
        "             \n",
        "    # End of 'if spark:' block\n",
        "    else:\n",
        "        print(\"Spark Session could not be initialized. Skipping DataFrame creation and Parquet writing.\")\n",
        "\n",
        "print(\"\\n--- Cell 3: Spark Operations Complete (or Skipped) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63bef2e",
      "metadata": {},
      "source": [
        "## Phase 2: Keras Model Training & Evaluation\n",
        "\n",
        "This phase loads the processed data from the Parquet files saved in Phase 1, defines the Keras model, trains it, and evaluates the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837ad551",
      "metadata": {},
      "source": [
        "### Cell 4: Load Processed Data & Prepare for Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d3856ff9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cell 4: Starting Keras Data Loading & Preparation ---\n",
            "Loading pre-processed data from Parquet: C:\\college\\CV\\COSMOS\\processed_cosmos_data_pyspark.parquet\n",
            "Loaded 896 sequences from Parquet into Pandas DataFrame.\n",
            "Columns found: ['sequence_id', 'input_features', 'target_cloud', 'target_convective', 'target_fog', 'target_moisture', 'target_thermo_contrast', 'target_temp_trend']\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 896 entries, 0 to 895\n",
            "Data columns (total 8 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   sequence_id             896 non-null    object \n",
            " 1   input_features          896 non-null    object \n",
            " 2   target_cloud            896 non-null    object \n",
            " 3   target_convective       896 non-null    object \n",
            " 4   target_fog              896 non-null    object \n",
            " 5   target_moisture         896 non-null    object \n",
            " 6   target_thermo_contrast  896 non-null    object \n",
            " 7   target_temp_trend       896 non-null    float64\n",
            "dtypes: float64(1), object(7)\n",
            "memory usage: 56.1+ KB\n",
            "\n",
            "--- Debugging: Inspecting first 3 rows as loaded from Parquet ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_id</th>\n",
              "      <th>input_features</th>\n",
              "      <th>target_cloud</th>\n",
              "      <th>target_convective</th>\n",
              "      <th>target_fog</th>\n",
              "      <th>target_moisture</th>\n",
              "      <th>target_thermo_contrast</th>\n",
              "      <th>target_temp_trend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3RIMG_17MAR2025_2145_L1C_SGP_V01R00.h5</td>\n",
              "      <td>[[[[0.86269468 0.84661466 0.77116781 0.88693094 0.01808104], [0.86269468 0.85231578 0.76826608 0.89674836 0.01903268], [0.86914611 0.86101615 0.76...</td>\n",
              "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0...</td>\n",
              "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0...</td>\n",
              "      <td>[[[1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0...</td>\n",
              "      <td>[[[0.7734650373458862], [0.774837076663971], [0.774837076663971], [0.7720751762390137], [0.7677935361862183], [0.7618001699447632], [0.75866925716...</td>\n",
              "      <td>[[[0.0008056640508584678], [0.005075531080365181], [0.009221648797392845], [0.0016748047200962901], [-0.014902038499712944], [-0.040740966796875],...</td>\n",
              "      <td>0.000539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3RIMG_15MAR2025_0045_L1C_SGP_V01R00.h5</td>\n",
              "      <td>[[[[0.98947299 0.98293984 0.84187162 0.99704498 0.02379085], [0.98947299 0.98293984 0.84187162 0.99752104 0.02616993], [0.98905891 0.98246956 0.84...</td>\n",
              "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...</td>\n",
              "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...</td>\n",
              "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...</td>\n",
              "      <td>[[[0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.843406856...</td>\n",
              "      <td>[[[-0.015034484677016735], [-0.019137268885970116], [-0.01655273512005806], [-0.01655273512005806], [-0.017922667786478996], [-0.01913726888597011...</td>\n",
              "      <td>0.005021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3RIMG_29MAR2025_1345_L1C_SGP_V01R00.h5</td>\n",
              "      <td>[[[[0.89207685 0.88802153 0.79686397 0.91115689 0.01522614], [0.89207685 0.88668936 0.79686397 0.90988117 0.01712941], [0.89207685 0.88534993 0.79...</td>\n",
              "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...</td>\n",
              "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...</td>\n",
              "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0...</td>\n",
              "      <td>[[[0.7765102386474609], [0.7765102386474609], [0.7778137922286987], [0.7791017889976501], [0.7803747057914734], [0.7828768491744995], [0.782876849...</td>\n",
              "      <td>[[[-0.03438857942819595], [-0.038124848157167435], [-0.02991577237844467], [-0.022195739671587944], [-0.02911636419594288], [-0.0331297293305397],...</td>\n",
              "      <td>-0.002559</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              sequence_id  \\\n",
              "0  3RIMG_17MAR2025_2145_L1C_SGP_V01R00.h5   \n",
              "1  3RIMG_15MAR2025_0045_L1C_SGP_V01R00.h5   \n",
              "2  3RIMG_29MAR2025_1345_L1C_SGP_V01R00.h5   \n",
              "\n",
              "                                                                                                                                          input_features  \\\n",
              "0  [[[[0.86269468 0.84661466 0.77116781 0.88693094 0.01808104], [0.86269468 0.85231578 0.76826608 0.89674836 0.01903268], [0.86914611 0.86101615 0.76...   \n",
              "1  [[[[0.98947299 0.98293984 0.84187162 0.99704498 0.02379085], [0.98947299 0.98293984 0.84187162 0.99752104 0.02616993], [0.98905891 0.98246956 0.84...   \n",
              "2  [[[[0.89207685 0.88802153 0.79686397 0.91115689 0.01522614], [0.89207685 0.88668936 0.79686397 0.90988117 0.01712941], [0.89207685 0.88534993 0.79...   \n",
              "\n",
              "                                                                                                                                            target_cloud  \\\n",
              "0  [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0...   \n",
              "1  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...   \n",
              "2  [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...   \n",
              "\n",
              "                                                                                                                                       target_convective  \\\n",
              "0  [[[1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0...   \n",
              "1  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...   \n",
              "2  [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...   \n",
              "\n",
              "                                                                                                                                              target_fog  \\\n",
              "0  [[[1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0...   \n",
              "1  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0...   \n",
              "2  [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0...   \n",
              "\n",
              "                                                                                                                                         target_moisture  \\\n",
              "0  [[[0.7734650373458862], [0.774837076663971], [0.774837076663971], [0.7720751762390137], [0.7677935361862183], [0.7618001699447632], [0.75866925716...   \n",
              "1  [[[0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.8426675200462341], [0.843406856...   \n",
              "2  [[[0.7765102386474609], [0.7765102386474609], [0.7778137922286987], [0.7791017889976501], [0.7803747057914734], [0.7828768491744995], [0.782876849...   \n",
              "\n",
              "                                                                                                                                  target_thermo_contrast  \\\n",
              "0  [[[0.0008056640508584678], [0.005075531080365181], [0.009221648797392845], [0.0016748047200962901], [-0.014902038499712944], [-0.040740966796875],...   \n",
              "1  [[[-0.015034484677016735], [-0.019137268885970116], [-0.01655273512005806], [-0.01655273512005806], [-0.017922667786478996], [-0.01913726888597011...   \n",
              "2  [[[-0.03438857942819595], [-0.038124848157167435], [-0.02991577237844467], [-0.022195739671587944], [-0.02911636419594288], [-0.0331297293305397],...   \n",
              "\n",
              "   target_temp_trend  \n",
              "0           0.000539  \n",
              "1           0.005021  \n",
              "2          -0.002559  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- End Debugging Inspection ---\n",
            "\n",
            "Converting loaded data to NumPy arrays...\n",
            "Attempting conversion for 'input_features'...\n",
            "Successfully converted 'input_features'. Shape: (896, 4, 32, 32, 5)\n",
            "Attempting conversion for 'cloud' (from column 'target_cloud')...\n",
            "Successfully converted 'cloud'. Shape: (896, 32, 32, 1)\n",
            "Attempting conversion for 'convective' (from column 'target_convective')...\n",
            "Successfully converted 'convective'. Shape: (896, 32, 32, 1)\n",
            "Attempting conversion for 'fog' (from column 'target_fog')...\n",
            "Successfully converted 'fog'. Shape: (896, 32, 32, 1)\n",
            "Attempting conversion for 'moisture' (from column 'target_moisture')...\n",
            "Successfully converted 'moisture'. Shape: (896, 32, 32, 1)\n",
            "Attempting conversion for 'thermo_contrast' (from column 'target_thermo_contrast')...\n",
            "Successfully converted 'thermo_contrast'. Shape: (896, 32, 32, 1)\n",
            "Attempting conversion for 'temp_trend' (from column 'target_temp_trend')...\n",
            "Successfully converted 'temp_trend'. Shape: (896, 1)\n",
            "\n",
            "Successfully converted all required columns to NumPy arrays with correct keys.\n",
            "Pandas DataFrame cleared from memory.\n",
            "\n",
            "Splitting data into training and validation sets (90/10 split)...\n",
            "Training set size: 806 samples\n",
            "Validation set size: 90 samples\n",
            "\n",
            "--- Final Shapes Before Exiting Cell ---\n",
            "X_train shape: (806, 4, 32, 32, 5), dtype: float32\n",
            "y_train keys: ['cloud', 'convective', 'fog', 'moisture', 'thermo_contrast', 'temp_trend']\n",
            "y_train['cloud'] shape: (806, 32, 32, 1), dtype: float32\n",
            "X_val shape: (90, 4, 32, 32, 5), dtype: float32\n",
            "y_val keys: ['cloud', 'convective', 'fog', 'moisture', 'thermo_contrast', 'temp_trend']\n",
            "y_val['cloud'] shape: (90, 32, 32, 1), dtype: float32\n",
            "------------------------------\n",
            "Full dataset arrays cleared from memory.\n",
            "\n",
            "--- Cell 4: Data Loading and Preparation Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 4: Load Processed Data & Prepare for Keras\n",
        "# ==============================================================\n",
        "print(\"\\n--- Cell 4: Starting Keras Data Loading & Preparation ---\")\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from IPython.display import display\n",
        "\n",
        "# --- Load Processed Data from Parquet ---\n",
        "# Uses PROCESSED_PARQUET_DIR defined in Cell 1\n",
        "if 'PROCESSED_PARQUET_DIR' not in locals():\n",
        "     PROCESSED_PARQUET_DIR = r\"C:\\college\\CV\\COSMOS\\processed_cosmos_data_pyspark.parquet\" # Fallback path\n",
        "     print(f\"Warning: PROCESSED_PARQUET_DIR not found, using default: {PROCESSED_PARQUET_DIR}\")\n",
        "# Ensure SEQ_LEN, PATCH_SIZE are defined\n",
        "if 'SEQ_LEN' not in locals(): SEQ_LEN = 4\n",
        "if 'PATCH_SIZE' not in locals(): PATCH_SIZE = 32\n",
        "\n",
        "\n",
        "print(f\"Loading pre-processed data from Parquet: {PROCESSED_PARQUET_DIR}\")\n",
        "if not os.path.exists(PROCESSED_PARQUET_DIR):\n",
        "     raise FileNotFoundError(f\"Parquet directory not found: {PROCESSED_PARQUET_DIR}. \"\n",
        "                           \"Ensure Phase 1 (PySpark - Cells 2 & 3) completed successfully and check the path.\")\n",
        "try:\n",
        "    # Use Pandas to read the Parquet directory (handles multiple files)\n",
        "    processed_df = pd.read_parquet(PROCESSED_PARQUET_DIR)\n",
        "    print(f\"Loaded {len(processed_df)} sequences from Parquet into Pandas DataFrame.\")\n",
        "    if processed_df.empty:\n",
        "         raise ValueError(\"Loaded DataFrame is empty! Check Parquet files generated in Cell 3.\")\n",
        "    print(\"Columns found:\", processed_df.columns.tolist())\n",
        "    print(\"DataFrame Info:\")\n",
        "    processed_df.info() \n",
        "\n",
        "    # --- Optional Debugging: Inspect Raw Loaded Data ---\n",
        "    print(\"\\n--- Debugging: Inspecting first 3 rows as loaded from Parquet ---\")\n",
        "    pd.set_option('display.max_colwidth', 150) \n",
        "    display(processed_df.head(3))\n",
        "    print(\"--- End Debugging Inspection ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error reading Parquet data with Pandas: {e}\")\n",
        "    raise \n",
        "\n",
        "\n",
        "# --- Convert Pandas DataFrame Columns to NumPy Arrays ---\n",
        "# Keras model.fit typically expects NumPy arrays\n",
        "print(\"\\nConverting loaded data to NumPy arrays...\")\n",
        "try:\n",
        "    # --- Conversion Attempt ---\n",
        "    print(\"Attempting conversion for 'input_features'...\")\n",
        "    # Convert each element (list) in the Series to a NumPy array, then stack them\n",
        "    # Add fallback for safety in case some rows failed processing earlier\n",
        "    X_data = np.stack(processed_df['input_features'].apply(lambda x: np.array(x, dtype=np.float32) if isinstance(x, list) and np.array(x).shape == (SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 5) else np.zeros((SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 5), dtype=np.float32)).values) \n",
        "    print(f\"Successfully converted 'input_features'. Shape: {X_data.shape}\") # Should be (N, 4, 32, 32, 5)\n",
        "\n",
        "    # --- Corrected Key Mapping for y_data --- \n",
        "    # Convert target variables into a dictionary of NumPy arrays\n",
        "    # Use keys that MATCH the model's output layer names\n",
        "    y_data = {}\n",
        "    target_mapping = {\n",
        "        'cloud': 'target_cloud', \n",
        "        'convective': 'target_convective', \n",
        "        'fog': 'target_fog',\n",
        "        'moisture': 'target_moisture', \n",
        "        'thermo_contrast': 'target_thermo_contrast',\n",
        "        'temp_trend': 'target_temp_trend'\n",
        "    }\n",
        "    \n",
        "    expected_target_shape = (PATCH_SIZE, PATCH_SIZE, 1)\n",
        "\n",
        "    for model_key, df_col in target_mapping.items():\n",
        "         if df_col in processed_df.columns:\n",
        "              print(f\"Attempting conversion for '{model_key}' (from column '{df_col}')...\")\n",
        "              if model_key == 'temp_trend': # Handle scalar target \n",
        "                   # Ensure it's float32 and correct shape\n",
        "                   y_data[model_key] = np.array(processed_df[df_col].tolist(), dtype=np.float32).reshape(-1, 1)\n",
        "              else: # Handle image-like targets\n",
        "                   # Apply conversion and stacking, with shape check\n",
        "                   y_data[model_key] = np.stack(processed_df[df_col].apply(lambda x: np.array(x, dtype=np.float32) if isinstance(x, list) and np.array(x).shape == expected_target_shape else np.zeros(expected_target_shape, dtype=np.float32)).values)\n",
        "              print(f\"Successfully converted '{model_key}'. Shape: {y_data[model_key].shape}\") # Should be (N, 32, 32, 1) or (N, 1)\n",
        "         else:\n",
        "              print(f\"Warning: Source column '{df_col}' for target '{model_key}' not found in DataFrame.\")\n",
        "              \n",
        "    # Verify all expected keys are in y_data\n",
        "    expected_keys = set(target_mapping.keys())\n",
        "    actual_keys = set(y_data.keys())\n",
        "    if expected_keys != actual_keys:\n",
        "         print(f\"Warning: Mismatch in created target keys. Expected {expected_keys}, got {actual_keys}\")\n",
        "    else:\n",
        "         print(\"\\nSuccessfully converted all required columns to NumPy arrays with correct keys.\")\n",
        "\n",
        "\n",
        "    # Optional: Free memory by deleting the Pandas DataFrame\n",
        "    del processed_df\n",
        "    gc.collect()\n",
        "    print(\"Pandas DataFrame cleared from memory.\")\n",
        "\n",
        "except KeyError as e:\n",
        "     print(f\"Error: Column '{e}' not found in the loaded Parquet data.\")\n",
        "     raise\n",
        "except ValueError as e:\n",
        "     # This error might now occur during np.stack if shapes are inconsistent between rows\n",
        "     print(f\"ValueError during conversion (np.stack): {e}\")\n",
        "     print(\"This likely indicates inconsistent shapes in the nested lists between different rows.\")\n",
        "     raise\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred converting Parquet data to NumPy: {e}\")\n",
        "    import traceback; traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# --- Train/Validation Split ---\n",
        "# Simulates the split logic from the original Cell 3\n",
        "print(\"\\nSplitting data into training and validation sets (90/10 split)...\")\n",
        "split_fraction = 0.9\n",
        "# Ensure X_data was successfully created before splitting\n",
        "if 'X_data' not in locals():\n",
        "     raise NameError(\"X_data was not created successfully in the conversion step.\")\n",
        "\n",
        "split_index = int(split_fraction * len(X_data))\n",
        "\n",
        "# Input features split\n",
        "X_train, X_val = X_data[:split_index], X_data[split_index:]\n",
        "\n",
        "# Target dictionary split\n",
        "y_train, y_val = {}, {}\n",
        "# Ensure y_data was successfully created\n",
        "if 'y_data' not in locals():\n",
        "    raise NameError(\"y_data dictionary was not created successfully in the conversion step.\")\n",
        "\n",
        "for key in y_data: # Iterate through keys in the created y_data dictionary\n",
        "    y_train[key] = y_data[key][:split_index]\n",
        "    y_val[key] = y_data[key][split_index:]\n",
        "\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Validation set size: {len(X_val)} samples\")\n",
        "\n",
        "# --- FINAL VERIFICATION PRINT ---\n",
        "print(\"\\n--- Final Shapes Before Exiting Cell ---\")\n",
        "print(f\"X_train shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
        "print(f\"y_train keys: {list(y_train.keys())}\") # Keys should NOT have 'target_' prefix\n",
        "if 'cloud' in y_train: print(f\"y_train['cloud'] shape: {y_train['cloud'].shape}, dtype: {y_train['cloud'].dtype}\")\n",
        "print(f\"X_val shape: {X_val.shape}, dtype: {X_val.dtype}\")\n",
        "print(f\"y_val keys: {list(y_val.keys())}\") # Keys should NOT have 'target_' prefix\n",
        "if 'cloud' in y_val: print(f\"y_val['cloud'] shape: {y_val['cloud'].shape}, dtype: {y_val['cloud'].dtype}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# Optional: Free memory from the original full arrays\n",
        "# Check if variables exist before deleting\n",
        "if 'X_data' in locals(): del X_data\n",
        "if 'y_data' in locals(): del y_data\n",
        "gc.collect()\n",
        "print(\"Full dataset arrays cleared from memory.\")\n",
        "\n",
        "print(\"\\n--- Cell 4: Data Loading and Preparation Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 5: Keras Model Definition & Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cell 5: Defining and Compiling Keras Model ---\n",
            "Defining Keras Multi-Output ConvLSTM model architecture...\n",
            "Model defined.\n",
            "Compiling Keras model (with original metrics)...\n",
            "Model compiled.\n",
            "\n",
            "Model Summary:\n",
            "Model: \"multitask_nowcast_notebook\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_features (InputLayer)    [(None, 4, 32, 32,   0           []                               \n",
            "                                5)]                                                               \n",
            "                                                                                                  \n",
            " convlstm_1 (ConvLSTM2D)        (None, 4, 32, 32, 3  42752       ['input_features[0][0]']         \n",
            "                                2)                                                                \n",
            "                                                                                                  \n",
            " batchnorm_1 (BatchNormalizatio  (None, 4, 32, 32, 3  128        ['convlstm_1[0][0]']             \n",
            " n)                             2)                                                                \n",
            "                                                                                                  \n",
            " convlstm_2 (ConvLSTM2D)        (None, 32, 32, 16)   27712       ['batchnorm_1[0][0]']            \n",
            "                                                                                                  \n",
            " batchnorm_2 (BatchNormalizatio  (None, 32, 32, 16)  64          ['convlstm_2[0][0]']             \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " global_avg_pool (GlobalAverage  (None, 16)          0           ['batchnorm_2[0][0]']            \n",
            " Pooling2D)                                                                                       \n",
            "                                                                                                  \n",
            " cloud (Conv2D)                 (None, 32, 32, 1)    17          ['batchnorm_2[0][0]']            \n",
            "                                                                                                  \n",
            " convective (Conv2D)            (None, 32, 32, 1)    17          ['batchnorm_2[0][0]']            \n",
            "                                                                                                  \n",
            " fog (Conv2D)                   (None, 32, 32, 1)    17          ['batchnorm_2[0][0]']            \n",
            "                                                                                                  \n",
            " moisture (Conv2D)              (None, 32, 32, 1)    17          ['batchnorm_2[0][0]']            \n",
            "                                                                                                  \n",
            " temp_trend (Dense)             (None, 1)            17          ['global_avg_pool[0][0]']        \n",
            "                                                                                                  \n",
            " thermo_contrast (Conv2D)       (None, 32, 32, 1)    17          ['batchnorm_2[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 70,758\n",
            "Trainable params: 70,662\n",
            "Non-trainable params: 96\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Using TensorFlow version: 2.8.0\n",
            "--- Cell 5: Model Definition and Compilation Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 5: Keras Model Definition & Compilation\n",
        "# ==============================================================\n",
        "print(\"\\n--- Cell 5: Defining and Compiling Keras Model ---\")\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np # Needed if redefining model after checkpoint load failure\n",
        "\n",
        "# Ensure config variables from Cell 1 are available\n",
        "if 'SEQ_LEN' not in locals(): SEQ_LEN = 4\n",
        "if 'PATCH_SIZE' not in locals(): PATCH_SIZE = 32\n",
        "# Define losses\n",
        "losses = {\n",
        "    'cloud': 'binary_crossentropy', 'convective': 'binary_crossentropy', 'fog': 'binary_crossentropy',\n",
        "    'moisture': 'mse', 'thermo_contrast': 'mse', 'temp_trend': 'mse'\n",
        "}\n",
        "# Define loss weights\n",
        "loss_weights = {'cloud': 1.0, 'convective': 1.0, 'fog': 1.0, 'moisture': 0.5, 'thermo_contrast': 0.5, 'temp_trend': 0.1}\n",
        "# Use Original Metrics dictionary (only 3 keys) - this matches your original notebook\n",
        "metrics = {'cloud':'accuracy','convective':'accuracy','fog':'accuracy'}\n",
        "\n",
        "\n",
        "# --- Model Definition (Matches original) ---\n",
        "print(\"Defining Keras Multi-Output ConvLSTM model architecture...\")\n",
        "# Define input layer shape based on prepared data\n",
        "# Shape is (TimeSteps, Height, Width, Channels)\n",
        "inp = layers.Input(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 5), name=\"input_features\") # Correct shape (4, 32, 32, 5)\n",
        "\n",
        "# ConvLSTM layers\n",
        "x = layers.ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True, activation='relu', name='convlstm_1')(inp)\n",
        "x = layers.BatchNormalization(name='batchnorm_1')(x)\n",
        "x = layers.ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False, activation='relu', name='convlstm_2')(x)\n",
        "x = layers.BatchNormalization(name='batchnorm_2')(x) # Output shape: (batch, H, W, 16)\n",
        "\n",
        "# Output heads for different prediction tasks\n",
        "# Ensure keys here match the keys in losses, loss_weights, and y_data dictionary\n",
        "heads = {\n",
        "    'cloud': layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='cloud')(x),\n",
        "    'convective': layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='convective')(x),\n",
        "    'fog': layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='fog')(x),\n",
        "    'moisture': layers.Conv2D(1, (1, 1), activation='linear', padding='same', name='moisture')(x),\n",
        "    'thermo_contrast': layers.Conv2D(1, (1, 1), activation='linear', padding='same', name='thermo_contrast')(x),\n",
        "}\n",
        "# Regression task for a single scalar value (temperature trend)\n",
        "temp_avg = layers.GlobalAveragePooling2D(name='global_avg_pool')(x) # Average spatial features -> (batch, 16)\n",
        "heads['temp_trend'] = layers.Dense(1, activation='linear', name='temp_trend')(temp_avg) # Dense layer for scalar output\n",
        "\n",
        "# Create the multi-output Keras Model\n",
        "model = Model(inputs=inp, outputs=heads, name='multitask_nowcast_notebook') # Use consistent name\n",
        "print(\"Model defined.\")\n",
        "\n",
        "# --- Model Compilation ---\n",
        "print(\"Compiling Keras model (with original metrics)...\")\n",
        "\n",
        "# Compile the model using the original metrics definition\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=losses,\n",
        "    loss_weights=loss_weights,\n",
        "    metrics=metrics # Use the metrics dict defined above (only 3 keys)\n",
        ")\n",
        "print(\"Model compiled.\")\n",
        "\n",
        "# --- Display Model Summary ---\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary() # Print layer information and parameter counts\n",
        "\n",
        "# --- Print TensorFlow Version ---\n",
        "print(f\"\\nUsing TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "\n",
        "print(\"--- Cell 5: Model Definition and Compilation Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 6: Keras Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cell 6: Starting Keras Model Training ---\n",
            "Setting up checkpoints in directory: checkpoints_pyspark_trained\n",
            "\n",
            "Attempting to load model from latest checkpoint: checkpoints_pyspark_trained\\model_epoch_20.h5\n",
            "Loading weights into existing model structure...\n",
            "Successfully loaded weights. Resuming training from epoch 20\n",
            "\n",
            "Training already completed up to epoch 20. Skipping training.\n",
            "\n",
            "--- Cell 6: Model Training Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 6: Keras Model Training\n",
        "# ==============================================================\n",
        "print(\"\\n--- Cell 6: Starting Keras Model Training ---\")\n",
        "\n",
        "# Ensure necessary imports are available if kernel restarted\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import gc # Ensure gc is imported if used\n",
        "\n",
        "# Ensure config variables from Cell 1 are available\n",
        "# Define defaults if running cell independently after kernel restart (adjust paths!)\n",
        "if 'CHECKPOINT_DIR' not in locals(): CHECKPOINT_DIR = \"checkpoints_pyspark_trained\"\n",
        "if 'EPOCHS' not in locals(): EPOCHS = 20\n",
        "if 'BATCH_SIZE' not in locals(): BATCH_SIZE = 16\n",
        "if 'MODEL_SAVE_PATH' not in locals(): MODEL_SAVE_PATH = r\"C:\\college\\CV\\COSMOS\\multitask_nowcast_pyspark_trained.h5\"\n",
        "# Ensure model from Cell 5 is available\n",
        "if 'model' not in locals(): raise NameError(\"Keras model not defined. Please run Cell 5 first.\")\n",
        "# Ensure data from Cell 4 is available\n",
        "if 'X_train' not in locals() or 'y_train' not in locals() or 'X_val' not in locals() or 'y_val' not in locals():\n",
        "     raise NameError(\"Training/Validation data not found. Please run the data preparation cell (Cell 4) first.\")\n",
        "# Get loss/metrics definitions from Cell 5 if available (needed for re-compile on load)\n",
        "if 'losses' not in locals():\n",
        "     losses = {\n",
        "         'cloud': 'binary_crossentropy', 'convective': 'binary_crossentropy', 'fog': 'binary_crossentropy',\n",
        "         'moisture': 'mse', 'thermo_contrast': 'mse', 'temp_trend': 'mse'\n",
        "     }\n",
        "if 'loss_weights' not in locals():\n",
        "     loss_weights = {'cloud': 1.0, 'convective': 1.0, 'fog': 1.0, 'moisture': 0.5, 'thermo_contrast': 0.5, 'temp_trend': 0.1}\n",
        "if 'metrics' not in locals(): # Use the original metrics definition\n",
        "     metrics = {'cloud':'accuracy','convective':'accuracy','fog':'accuracy'}\n",
        "\n",
        "\n",
        "# --- Setup Checkpointing ---\n",
        "print(f\"Setting up checkpoints in directory: {CHECKPOINT_DIR}\")\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True) # Ensure checkpoint directory exists\n",
        "\n",
        "# Callback to save the model after each epoch\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(CHECKPOINT_DIR, \"model_epoch_{epoch:02d}.h5\"),\n",
        "    save_weights_only=False, # Save the full model\n",
        "    save_freq='epoch',       # Save at the end of each epoch\n",
        "    verbose=1                # Print message when saving\n",
        ")\n",
        "\n",
        "# --- Resume Logic ---\n",
        "initial_epoch = 0 # Default starting epoch\n",
        "ckpt_files = glob.glob(os.path.join(CHECKPOINT_DIR, \"*.h5\"))\n",
        "\n",
        "if ckpt_files:\n",
        "    # Sort checkpoints by epoch number\n",
        "    def get_epoch_num(fpath):\n",
        "        match = re.search(r\"epoch_(\\d+)\", os.path.basename(fpath))\n",
        "        return int(match.group(1)) if match else -1\n",
        "        \n",
        "    ckpt_files.sort(key=get_epoch_num)\n",
        "    latest_ckpt = ckpt_files[-1]\n",
        "    latest_epoch_num = get_epoch_num(latest_ckpt)\n",
        "    \n",
        "    if latest_epoch_num != -1:\n",
        "        print(f\"\\nAttempting to load model from latest checkpoint: {latest_ckpt}\")\n",
        "        try:\n",
        "            # Load weights into the existing 'model' structure defined in Cell 5\n",
        "            print(\"Loading weights into existing model structure...\")\n",
        "            model.load_weights(latest_ckpt) # Load only weights\n",
        "            initial_epoch = latest_epoch_num # Keras initial_epoch starts AT this epoch number\n",
        "            print(f\"Successfully loaded weights. Resuming training from epoch {initial_epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error loading checkpoint weights {latest_ckpt}: {e}.\")\n",
        "            print(\"Training from scratch.\")\n",
        "            initial_epoch = 0\n",
        "    else:\n",
        "        print(f\"Warning: Could not determine epoch number from latest checkpoint file '{latest_ckpt}'. Training from scratch.\")\n",
        "else:\n",
        "    print(\"No checkpoints found. Training model from scratch.\")\n",
        "\n",
        "\n",
        "# --- Start or Resume Training ---\n",
        "if initial_epoch < EPOCHS:\n",
        "    print(f\"\\n--- Verifying Data Before Fit (Epoch {initial_epoch} to {EPOCHS}) ---\")\n",
        "    # Basic checks on data passed to fit\n",
        "    print(f\"X_train shape: {X_train.shape}, dtype: {X_train.dtype}\") \n",
        "    print(f\"y_train keys: {list(y_train.keys())}\") # Should NOT have 'target_' prefix\n",
        "    if 'cloud' in y_train:\n",
        "        print(f\"y_train['cloud'] shape: {y_train['cloud'].shape}, dtype: {y_train['cloud'].dtype}\")\n",
        "    print(f\"X_val shape: {X_val.shape}, dtype: {X_val.dtype}\") \n",
        "    print(f\"y_val keys: {list(y_val.keys())}\") # Should NOT have 'target_' prefix\n",
        "    if 'cloud' in y_val:\n",
        "        print(f\"y_val['cloud'] shape: {y_val['cloud'].shape}, dtype: {y_val['cloud'].dtype}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # --- Check for NaN/Inf values ---\n",
        "    print(\"Checking for NaN/Inf values in target data...\")\n",
        "    abort_training = False\n",
        "    for dataset_name, dataset_dict in [(\"y_train\", y_train), (\"y_val\", y_val)]:\n",
        "        for key, arr in dataset_dict.items():\n",
        "            if arr is not None:\n",
        "                if np.isnan(arr).any():\n",
        "                    print(f\"  Error: NaN values found in {dataset_name}['{key}']\")\n",
        "                    abort_training = True\n",
        "                if np.isinf(arr).any():\n",
        "                    print(f\"  Error: Infinite values found in {dataset_name}['{key}']\")\n",
        "                    abort_training = True\n",
        "            else:\n",
        "                print(f\"  Error: {dataset_name}['{key}'] is None.\")\n",
        "                abort_training = True\n",
        "    if abort_training:\n",
        "        raise ValueError(\"NaN or Infinite values found in target data. Cannot proceed.\")\n",
        "    else:\n",
        "        print(\"No NaN or Infinite values found in target data.\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # --- Check model output names match y_train keys ---\n",
        "    print(\"Verifying model output names match target keys...\")\n",
        "    model_output_names = model.output_names\n",
        "    y_train_keys = list(y_train.keys())\n",
        "    print(f\"  Model output names: {sorted(model_output_names)}\")\n",
        "    print(f\"  y_train keys:       {sorted(y_train_keys)}\")\n",
        "    if sorted(model_output_names) != sorted(y_train_keys):\n",
        "        # This check should now PASS\n",
        "        raise ValueError(\"Mismatch between model output layer names and keys in the y_train dictionary!\")\n",
        "    else:\n",
        "        print(\"Model output names match target keys.\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    \n",
        "    print(f\"\\nStarting model.fit from epoch {initial_epoch}...\")\n",
        "    history = None # Initialize history\n",
        "    try:\n",
        "        # *** Pass NumPy arrays directly to model.fit ***\n",
        "        history = model.fit(\n",
        "            X_train, y_train,                 # Training data and labels (y_train is a dict)\n",
        "            validation_data=(X_val, y_val),   # Validation data and labels (y_val is a dict)\n",
        "            batch_size=BATCH_SIZE,            # Number of samples per gradient update\n",
        "            epochs=EPOCHS,                    # Total number of epochs to train for\n",
        "            callbacks=[checkpoint_callback],  # List of callbacks (e.g., for saving)\n",
        "            initial_epoch=initial_epoch,      # Starting epoch (for resuming)\n",
        "            verbose=1                         # Show progress bar (1) or epoch summary (2)\n",
        "        )\n",
        "        print(\"Model training finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError occurred during model.fit: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback if fit fails\n",
        "        raise # Re-raise the error to stop execution\n",
        "\n",
        "\n",
        "    # --- Save the Final Model (only if training completed) ---\n",
        "    if history is not None: # Check if training actually ran\n",
        "        print(f\"\\nSaving final trained model to: {MODEL_SAVE_PATH}\")\n",
        "        try:\n",
        "            model.save(MODEL_SAVE_PATH)\n",
        "            print(\"Final model saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving final model: {e}\")\n",
        "            \n",
        "        # --- Plot Training History (Loss and Accuracy) ---\n",
        "        print(\"\\nPlotting training history...\")\n",
        "        history_dict = history.history\n",
        "        \n",
        "        # Determine available metrics from history\n",
        "        loss_keys = [k for k in history_dict if k.startswith('loss') or k.startswith('val_loss')]\n",
        "        # Accuracy keys should exist because we compiled with them in Cell 5\n",
        "        acc_keys = [k for k in history_dict if k.startswith('accuracy') or k.startswith('val_') and 'accuracy' in k]\n",
        "        \n",
        "        # Calculate the actual range of epochs trained in this run\n",
        "        num_epochs_trained = len(history_dict.get('loss', [])) # Use .get for safety\n",
        "        if num_epochs_trained > 0:\n",
        "             actual_epochs_range = range(initial_epoch, initial_epoch + num_epochs_trained)\n",
        "             \n",
        "             plt.style.use('seaborn-v0_8-darkgrid') # Use a visually appealing style\n",
        "             \n",
        "             # Plot Total Loss\n",
        "             if 'loss' in history_dict and 'val_loss' in history_dict:\n",
        "                  # Check if accuracy keys exist to decide subplot layout\n",
        "                  fig_cols = 2 if acc_keys else 1\n",
        "                  plt.figure(figsize=(7 * fig_cols, 6)) # Adjust figure size\n",
        "                  plt.subplot(1, fig_cols, 1)\n",
        "                  plt.plot(actual_epochs_range, history_dict['loss'], 'o-', label='Training Loss', linewidth=2)\n",
        "                  plt.plot(actual_epochs_range, history_dict['val_loss'], 's--', label='Validation Loss', linewidth=2)\n",
        "                  plt.title('Total Training and Validation Loss', fontsize=14)\n",
        "                  plt.xlabel('Epoch', fontsize=12)\n",
        "                  plt.ylabel('Loss', fontsize=12)\n",
        "                  plt.legend(fontsize=10)\n",
        "                  plt.grid(True, linestyle=':')\n",
        "             \n",
        "             # Plot Accuracy (only if keys exist)\n",
        "             train_acc_key = next((k for k in history_dict if k.endswith('accuracy') and not k.startswith('val_')), None)\n",
        "             val_acc_key = next((k for k in history_dict if k.startswith('val_') and k.endswith('accuracy')), None)\n",
        "             \n",
        "             if train_acc_key and val_acc_key:\n",
        "                  # Add subplot for accuracy if loss was plotted\n",
        "                  if fig_cols == 2:\n",
        "                       plt.subplot(1, 2, 2)\n",
        "                  else: # Otherwise create a new figure\n",
        "                       plt.figure(figsize=(7, 6))\n",
        "                       \n",
        "                  plt.plot(actual_epochs_range, history_dict[train_acc_key], 'o-', label=f'Training Accuracy ({train_acc_key})', linewidth=2)\n",
        "                  plt.plot(actual_epochs_range, history_dict[val_acc_key], 's--', label=f'Validation Accuracy ({val_acc_key})', linewidth=2)\n",
        "                  plt.title('Training and Validation Accuracy', fontsize=14)\n",
        "                  plt.xlabel('Epoch', fontsize=12)\n",
        "                  plt.ylabel('Accuracy', fontsize=12)\n",
        "                  plt.legend(fontsize=10)\n",
        "                  plt.grid(True, linestyle=':')\n",
        "             \n",
        "             plt.tight_layout() # Adjust layout\n",
        "             plt.show() # Display the plot(s)\n",
        "        else:\n",
        "             print(\"No training history found to plot (training might not have run).\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\nTraining already completed up to epoch {initial_epoch}. Skipping training.\")\n",
        "    # If training was skipped, you might want to load the history object if saved previously\n",
        "    # history = ... # load history if needed for plotting\n",
        "\n",
        "print(\"\\n--- Cell 6: Model Training Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 7: Keras Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Cell 7: Evaluating Final Model ---\n",
            "Loading final model from: C:\\college\\CV\\COSMOS\\multitask_nowcast_pyspark_trained.h5\n",
            "Model loaded successfully for evaluation.\n",
            "\n",
            "Running model.evaluate on validation data...\n",
            "6/6 [==============================] - 6s 34ms/step - loss: 0.0149 - cloud_loss: 0.0069 - convective_loss: 0.0032 - fog_loss: 0.0048 - moisture_loss: 4.2593e-12 - temp_trend_loss: 2.5247e-05 - thermo_contrast_loss: 8.6701e-11 - cloud_accuracy: 1.0000 - convective_accuracy: 1.0000 - fog_accuracy: 1.0000\n",
            "\n",
            "Model Evaluation Results (from model.evaluate):\n",
            "  loss: 0.0149\n",
            "  cloud_loss: 0.0069\n",
            "  convective_loss: 0.0032\n",
            "  fog_loss: 0.0048\n",
            "  moisture_loss: 0.0000\n",
            "  temp_trend_loss: 0.0000\n",
            "  thermo_contrast_loss: 0.0000\n",
            "  cloud_accuracy: 1.0000\n",
            "  convective_accuracy: 1.0000\n",
            "  fog_accuracy: 1.0000\n",
            "\n",
            "Calculating detailed metrics manually...\n",
            "Generating predictions for 90 validation samples...\n",
            "6/6 [==============================] - 0s 12ms/step\n",
            "Predictions generated.\n",
            "\n",
            "--- Segmentation Metrics ---\n",
            "An unexpected error occurred during manual evaluation: 'seaborn-v0_8-whitegrid' not found in the style library and input is not a valid URL or path; see `style.available` for list of available styles\n",
            "\n",
            "--- Cell 7: Model Evaluation Complete ---\n",
            "\n",
            "--- Full Notebook Workflow Finished ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\matplotlib\\style\\core.py\", line 127, in use\n",
            "    rc = rc_params_from_file(style, use_default_template=False)\n",
            "  File \"c:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\matplotlib\\__init__.py\", line 854, in rc_params_from_file\n",
            "    config_from_file = _rc_params_in_file(fname, fail_on_error=fail_on_error)\n",
            "  File \"c:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\matplotlib\\__init__.py\", line 780, in _rc_params_in_file\n",
            "    with _open_file_or_url(fname) as fd:\n",
            "  File \"c:\\Users\\dhanu\\.conda\\envs\\w\\lib\\contextlib.py\", line 119, in __enter__\n",
            "    return next(self.gen)\n",
            "  File \"c:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\matplotlib\\__init__.py\", line 757, in _open_file_or_url\n",
            "    with open(fname, encoding=encoding) as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'seaborn-v0_8-whitegrid'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_17968\\1947837374.py\", line 95, in <module>\n",
            "    plt.style.use('seaborn-v0_8-whitegrid') # Style for plots\n",
            "  File \"c:\\Users\\dhanu\\.conda\\envs\\w\\lib\\site-packages\\matplotlib\\style\\core.py\", line 130, in use\n",
            "    raise IOError(\n",
            "OSError: 'seaborn-v0_8-whitegrid' not found in the style library and input is not a valid URL or path; see `style.available` for list of available styles\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Cell 7: Keras Model Evaluation\n",
        "# ==============================================================\n",
        "print(\"\\n--- Cell 7: Evaluating Final Model ---\")\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "import os\n",
        "import re # Needed for checkpoint loading fallback\n",
        "import glob # Needed for checkpoint loading fallback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, precision_score, recall_score,\n",
        "    f1_score, accuracy_score, mean_squared_error, mean_absolute_error\n",
        ")\n",
        "from IPython.display import display\n",
        "from tqdm.notebook import tqdm # Use notebook version\n",
        "import gc\n",
        "\n",
        "# Ensure config variables from Cell 1 are available\n",
        "if 'MODEL_SAVE_PATH' not in locals(): MODEL_SAVE_PATH = r\"C:\\college\\CV\\COSMOS\\multitask_nowcast_pyspark_trained.h5\" # Match save path\n",
        "if 'BATCH_SIZE' not in locals(): BATCH_SIZE = 16\n",
        "# Ensure validation data from Cell 4 is available\n",
        "if 'X_val' not in locals() or 'y_val' not in locals():\n",
        "     raise NameError(\"Validation data (X_val, y_val) not found. Please run the data preparation cell (Cell 4) first.\")\n",
        "if 'CHECKPOINT_DIR' not in locals(): CHECKPOINT_DIR = \"checkpoints_pyspark_trained\" # Match checkpoint dir\n",
        "\n",
        "\n",
        "# --- Load the Final Saved Model --- \n",
        "# Ensure evaluation is done on the definitive final model saved after training\n",
        "print(f\"Loading final model from: {MODEL_SAVE_PATH}\")\n",
        "if not os.path.exists(MODEL_SAVE_PATH):\n",
        "    # Try loading from the last checkpoint if final model doesn't exist\n",
        "    print(f\"Warning: Final model not found at {MODEL_SAVE_PATH}. Trying latest checkpoint...\")\n",
        "    ckpt_files = sorted(glob.glob(os.path.join(CHECKPOINT_DIR, \"*.h5\")),\n",
        "                        key=lambda f: int(re.search(r\"epoch_(\\d+)\", os.path.basename(f)).group(1) if re.search(r\"epoch_(\\d+)\", os.path.basename(f)) else -1))\n",
        "    if not ckpt_files:\n",
        "        raise FileNotFoundError(f\"No final model or checkpoints found. Please ensure training completed.\")\n",
        "    latest_ckpt = ckpt_files[-1]\n",
        "    print(f\"Loading model from latest checkpoint: {latest_ckpt}\")\n",
        "    MODEL_TO_LOAD = latest_ckpt\n",
        "else:\n",
        "    MODEL_TO_LOAD = MODEL_SAVE_PATH\n",
        "\n",
        "try:\n",
        "    # Load the model. compile=True is needed for model.evaluate with compiled metrics.\n",
        "    eval_model = tf.keras.models.load_model(MODEL_TO_LOAD, compile=True) \n",
        "    print(\"Model loaded successfully for evaluation.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Option 1: Use model.evaluate() --- \n",
        "# Provides a quick summary based on compiled metrics and losses\n",
        "print(\"\\nRunning model.evaluate on validation data...\")\n",
        "try:\n",
        "    # Pass validation data directly as NumPy arrays\n",
        "    results = eval_model.evaluate(X_val, y_val, batch_size=BATCH_SIZE, verbose=1, return_dict=True)\n",
        "    print(\"\\nModel Evaluation Results (from model.evaluate):\")\n",
        "    # Format the results nicely\n",
        "    for name, value in results.items():\n",
        "        print(f\"  {name}: {value:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during model.evaluate: {e}\")\n",
        "\n",
        "# --- Option 2: Manual Detailed Evaluation --- \n",
        "# Calculates metrics individually, allowing more control and custom metrics\n",
        "print(\"\\nCalculating detailed metrics manually...\")\n",
        "try:\n",
        "    # Get predictions on the validation set\n",
        "    print(f\"Generating predictions for {len(X_val)} validation samples...\")\n",
        "    # Use predict on NumPy array\n",
        "    y_pred_val_dict = eval_model.predict(X_val, batch_size=BATCH_SIZE, verbose=1)\n",
        "    # Ensure predictions are in a dictionary if model has multiple named outputs\n",
        "    if not isinstance(y_pred_val_dict, dict):\n",
        "         # If predict returns a list, try to map it to output names\n",
        "         if isinstance(y_pred_val_dict, list) and len(y_pred_val_dict) == len(eval_model.output_names):\n",
        "              y_pred_val_dict = dict(zip(eval_model.output_names, y_pred_val_dict))\n",
        "         else:\n",
        "              # If single output model, wrap it in a dict\n",
        "              if len(eval_model.output_names) == 1:\n",
        "                   y_pred_val_dict = {eval_model.output_names[0]: y_pred_val_dict}\n",
        "              else:\n",
        "                   raise TypeError(f\"Model prediction output type is {type(y_pred_val_dict)}, expected dict or list matching output names.\")\n",
        "                   \n",
        "    print(\"Predictions generated.\")\n",
        "    \n",
        "    # --- Calculate Segmentation Metrics ---\n",
        "    seg_keys = [\"cloud\", \"convective\", \"fog\"]\n",
        "    conf_matrices = {}\n",
        "    seg_metrics_list = []\n",
        "    print(\"\\n--- Segmentation Metrics ---\")\n",
        "    plt.style.use('seaborn-v0_8-whitegrid') # Style for plots\n",
        "    \n",
        "    for k in seg_keys:\n",
        "        if k not in y_val or k not in y_pred_val_dict:\n",
        "             print(f\"  Skipping metric calculation for '{k}': Key not found in y_val or predictions.\")\n",
        "             continue\n",
        "        print(f\"  Calculating for: {k}\")\n",
        "        # Flatten true labels and predictions for comparison\n",
        "        y_true_flat = y_val[k].flatten().astype(np.uint8)\n",
        "        # Apply threshold (0.5) to sigmoid output for binary prediction\n",
        "        y_pred_flat = (y_pred_val_dict[k].flatten() > 0.5).astype(np.uint8) \n",
        "        \n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1]) # Explicitly use labels 0 and 1\n",
        "        conf_matrices[k] = cm\n",
        "        \n",
        "        # Handle potential division by zero if a class is missing or never predicted\n",
        "        try:\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "        except ValueError: # Handle case where cm might not be 2x2\n",
        "             print(f\"    Warning: Confusion matrix for '{k}' is not 2x2 ({cm.shape}). Metrics might be affected.\")\n",
        "             TN, FP, FN, TP = 0, 0, 0, 0\n",
        "             if np.sum(y_true_flat == 0) == len(y_true_flat): TN = np.sum((y_true_flat==0)&(y_pred_flat==0)); FP = np.sum((y_true_flat==0)&(y_pred_flat==1))\n",
        "             elif np.sum(y_true_flat == 1) == len(y_true_flat): TP = np.sum((y_true_flat==1)&(y_pred_flat==1)); FN = np.sum((y_true_flat==1)&(y_pred_flat==0))\n",
        "                 \n",
        "        # Calculate standard metrics\n",
        "        acc = accuracy_score(y_true_flat, y_pred_flat)\n",
        "        # Use zero_division=0 to return 0 instead of nan/error if denominator is zero\n",
        "        prec = precision_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "        rec = recall_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "        f1 = f1_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "        \n",
        "        seg_metrics_list.append(dict(Task=k, Acc=acc, Prec=prec, Rec=rec, F1=f1, TN=TN, FP=FP, FN=FN, TP=TP))\n",
        "    \n",
        "    # Display segmentation metrics in a formatted table\n",
        "    if seg_metrics_list:\n",
        "        df_seg_metrics = pd.DataFrame(seg_metrics_list).set_index(\"Task\")\n",
        "        # Format floats to 4 decimal places for readability\n",
        "        pd.options.display.float_format = '{:.4f}'.format \n",
        "        print(\"\\nSegmentation Metrics Summary:\")\n",
        "        display(df_seg_metrics[['Acc', 'Prec', 'Rec', 'F1']]) # Use display for better notebook formatting\n",
        "        # print(df_seg_metrics) # Uncomment to see TN, FP, FN, TP counts\n",
        "    else:\n",
        "        print(\"No segmentation metrics calculated.\")\n",
        "        \n",
        "    # --- Calculate Regression Metrics ---\n",
        "    reg_keys = [\"moisture\", \"thermo_contrast\", \"temp_trend\"]\n",
        "    reg_metrics_list = []\n",
        "    print(\"\\n--- Regression Metrics ---\")\n",
        "    for k in reg_keys:\n",
        "        if k not in y_val or k not in y_pred_val_dict:\n",
        "             print(f\"  Skipping metric calculation for '{k}': Key not found in y_val or predictions.\")\n",
        "             continue\n",
        "        print(f\"  Calculating for: {k}\")\n",
        "        y_true_flat = y_val[k].flatten()\n",
        "        y_pred_flat = y_pred_val_dict[k].flatten()\n",
        "        \n",
        "        mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
        "        mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
        "        reg_metrics_list.append(dict(Task=k, MSE=mse, MAE=mae))\n",
        "    \n",
        "    # Display regression metrics in a formatted table\n",
        "    if reg_metrics_list:\n",
        "        df_reg_metrics = pd.DataFrame(reg_metrics_list).set_index(\"Task\")\n",
        "        print(\"\\nRegression Metrics Summary:\")\n",
        "        display(df_reg_metrics)\n",
        "    else:\n",
        "         print(\"No regression metrics calculated.\")\n",
        "         \n",
        "    # --- Plot Confusion Matrices (Visuals) ---\n",
        "    print(\"\\n--- Confusion Matrices (Visualizations) ---\")\n",
        "    # Set a consistent color map and style\n",
        "    cmap = plt.cm.Blues \n",
        "    plt.style.use('seaborn-v0_8-darkgrid') # Use darkgrid for better contrast\n",
        "    \n",
        "    for k, cm in conf_matrices.items():\n",
        "        fig, ax = plt.subplots(figsize=(6, 5)) # Slightly larger figure for clarity\n",
        "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap) \n",
        "        ax.figure.colorbar(im, ax=ax, shrink=0.8) # Add colorbar\n",
        "        \n",
        "        # Configure axes, labels, title\n",
        "        ax.set(xticks=np.arange(cm.shape[1]),\n",
        "               yticks=np.arange(cm.shape[0]),\n",
        "               xticklabels=[\"Predicted 0\", \"Predicted 1\"], \n",
        "               yticklabels=[\"True 0\", \"True 1\"],\n",
        "               title=f'{k.capitalize()} Confusion Matrix',\n",
        "               ylabel='True label',\n",
        "               xlabel='Predicted label')\n",
        "        ax.title.set_fontsize(14)\n",
        "        ax.xaxis.label.set_fontsize(12)\n",
        "        ax.yaxis.label.set_fontsize(12)\n",
        "\n",
        "        # Add text annotations within each cell\n",
        "        fmt = 'd' # Format as integer\n",
        "        thresh = cm.max() / 2. # Threshold for text color (white/black)\n",
        "        # Add text annotations for each cell value\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(j, i, format(cm[i, j], fmt),\n",
        "                        fontsize=11,\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        # Set text color based on background\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "                        \n",
        "        fig.tight_layout() # Adjust layout\n",
        "        plt.show() # Display the plot for each matrix\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"Error during manual evaluation: {e}\")\n",
        "    print(\"Please ensure Cell 4 (Data Prep) and Cell 5/6 (Training/Prediction) executed correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during manual evaluation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Cell 7: Model Evaluation Complete ---\")\n",
        "print(\"\\n--- Full Notebook Workflow Finished ---\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "w",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
