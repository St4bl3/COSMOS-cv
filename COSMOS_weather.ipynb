{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe64440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6\n",
      "Loaded zlibwapi.dll\n",
      "Using 1 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, Mixed Precision, GPU & DLL Setup, Configuration\n",
    "\n",
    "import os, glob, ctypes\n",
    "import h5py, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, mixed_precision\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, accuracy_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Enable mixed precision for speed on GPU\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# DLL fix for Windows HDF5/zlib\n",
    "try:\n",
    "    dll = os.path.join(os.environ['CONDA_PREFIX'], 'Library', 'bin', 'zlibwapi.dll')\n",
    "    ctypes.CDLL(dll)\n",
    "    print(\"Loaded zlibwapi.dll\")\n",
    "except Exception:\n",
    "    print(\"Could not load zlibwapi.dll\")\n",
    "\n",
    "# GPU config\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"Using {len(gpus)} GPU(s)\")\n",
    "else:\n",
    "    print(\"No GPU detected\")\n",
    "\n",
    "# Config â€” tuned for speed\n",
    "DATA_DIR      = r\"C:\\college\\CV\\COSMOS\\6C_full\"\n",
    "SEQ_LEN       = 4         # use 4 time steps instead of 6\n",
    "PATCH_SIZE    = 32        # use 32Ã—32 patches instead of 64Ã—64\n",
    "BATCH_SIZE    = 16        # larger batch if GPU memory allows\n",
    "EPOCHS        = 20        # max epochs\n",
    "THRESHOLD     = 265.0\n",
    "CV_THRESHOLD  = 260.0\n",
    "FOG_THRESHOLD = 270.0\n",
    "MODEL_PATH    = r\"C:\\college\\CV\\COSMOS\\multitask_nowcast_fast.h5\"\n",
    "\n",
    "# list files & build sliding windows\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.h5\")))\n",
    "sequences = [all_files[i:i+SEQ_LEN+1] for i in range(len(all_files)-SEQ_LEN)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f3700",
   "metadata": {},
   "source": [
    "Data-Loading & Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b3f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loader & Generator\n",
    "\n",
    "def load_multi(fp_seq):\n",
    "    frames = []\n",
    "    for fp in fp_seq[:SEQ_LEN]:\n",
    "        with h5py.File(fp,'r') as f:\n",
    "            cnt1, cnt2   = f['IMG_TIR1'][0][...], f['IMG_TIR2'][0][...]\n",
    "            cnt_wv, cnt_mir = f['IMG_WV'][0][...], f['IMG_MIR'][0][...]\n",
    "            cnt_vis      = f['IMG_VIS'][0][...]\n",
    "            lut1, lut2   = f['IMG_TIR1_TEMP'][:],  f['IMG_TIR2_TEMP'][:]\n",
    "            lut_wv, lut_mir = f['IMG_WV_TEMP'][:], f['IMG_MIR_TEMP'][:]\n",
    "            lut_vis      = f['IMG_VIS_ALBEDO'][:]\n",
    "        bt1 = lut1[cnt1]; bt2 = lut2[cnt2]\n",
    "        wv  = lut_wv[cnt_wv]; mir = lut_mir[cnt_mir]\n",
    "        vis = lut_vis[cnt_vis]\n",
    "        frames.append(np.stack([bt1,bt2,wv,mir,vis],axis=-1)/300.0)\n",
    "    X = np.stack(frames,axis=0).astype(np.float32)\n",
    "\n",
    "    with h5py.File(fp_seq[-1],'r') as f:\n",
    "        cnt1, cnt2   = f['IMG_TIR1'][0][...], f['IMG_TIR2'][0][...]\n",
    "        cnt_wv, cnt_mir = f['IMG_WV'][0][...], f['IMG_MIR'][0][...]\n",
    "        lut1, lut2   = f['IMG_TIR1_TEMP'][:],  f['IMG_TIR2_TEMP'][:]\n",
    "        lut_wv, lut_mir = f['IMG_WV_TEMP'][:], f['IMG_MIR_TEMP'][:]\n",
    "    bt1_t = lut1[cnt1]; bt2_t = lut2[cnt2]\n",
    "    wv_t  = lut_wv[cnt_wv]; mir_t = lut_mir[cnt_mir]\n",
    "\n",
    "    # temperature trend normalized\n",
    "    last_mean  = bt1_t.mean()/300.0\n",
    "    first_mean = X[0,...,0].mean()\n",
    "    temp_trend = np.array([last_mean - first_mean],dtype=np.float32)\n",
    "\n",
    "    return X, {\n",
    "        'cloud'          : (bt1_t<THRESHOLD).astype(np.float32)[...,None],\n",
    "        'convective'     : (bt1_t<CV_THRESHOLD).astype(np.float32)[...,None],\n",
    "        'fog'            : (mir_t<FOG_THRESHOLD).astype(np.float32)[...,None],\n",
    "        'moisture'       : (wv_t/300.0).astype(np.float32)[...,None],\n",
    "        'thermo_contrast': ((bt2_t-bt1_t)/100.0).astype(np.float32)[...,None],\n",
    "        'temp_trend'     : temp_trend\n",
    "    }\n",
    "\n",
    "def random_crop(X,y):\n",
    "    H,W = X.shape[1], X.shape[2]\n",
    "    i,j = np.random.randint(0,H-PATCH_SIZE), np.random.randint(0,W-PATCH_SIZE)\n",
    "    Xc = X[:,i:i+PATCH_SIZE,j:j+PATCH_SIZE,:]\n",
    "    yc = {}\n",
    "    for k,v in y.items():\n",
    "        yc[k] = v[i:i+PATCH_SIZE,j:j+PATCH_SIZE] if v.ndim==3 else v\n",
    "    return Xc,yc\n",
    "\n",
    "def generator(seqs):\n",
    "    while True:\n",
    "        np.random.shuffle(seqs)\n",
    "        for seq in seqs:\n",
    "            X,y = load_multi(seq)\n",
    "            yield random_crop(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd47b4",
   "metadata": {},
   "source": [
    "Dataset Split & steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27a1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset Split, Caching & Pipelines\n",
    "\n",
    "# train/val split\n",
    "split       = int(0.9*len(sequences))\n",
    "train_seqs  = sequences[:split]\n",
    "val_seqs    = sequences[split:]\n",
    "\n",
    "# steps per epoch\n",
    "train_steps = len(train_seqs)//BATCH_SIZE\n",
    "val_steps   = len(val_seqs)//BATCH_SIZE\n",
    "\n",
    "# common output signature\n",
    "output_signature = (\n",
    "    tf.TensorSpec((SEQ_LEN,PATCH_SIZE,PATCH_SIZE,5),tf.float32),\n",
    "    {\n",
    "      'cloud': tf.TensorSpec((PATCH_SIZE,PATCH_SIZE,1),tf.float32),\n",
    "      'convective': tf.TensorSpec((PATCH_SIZE,PATCH_SIZE,1),tf.float32),\n",
    "      'fog': tf.TensorSpec((PATCH_SIZE,PATCH_SIZE,1),tf.float32),\n",
    "      'moisture': tf.TensorSpec((PATCH_SIZE,PATCH_SIZE,1),tf.float32),\n",
    "      'thermo_contrast': tf.TensorSpec((PATCH_SIZE,PATCH_SIZE,1),tf.float32),\n",
    "      'temp_trend': tf.TensorSpec((1,),tf.float32)\n",
    "    }\n",
    ")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(train_seqs), output_signature=output_signature\n",
    ").cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(val_seqs), output_signature=output_signature\n",
    ").cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8115447",
   "metadata": {},
   "source": [
    "Cell 4: Model Definition & Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fee617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fast_multitask_nowcast\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 32, 32,   0           []                               \n",
      "                                5)]                                                               \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)       (None, 4, 32, 32, 3  42752       ['input_1[0][0]']                \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 4, 32, 32, 3  128        ['conv_lstm2d[0][0]']            \n",
      " alization)                     2)                                                                \n",
      "                                                                                                  \n",
      " conv_lstm2d_1 (ConvLSTM2D)     (None, 32, 32, 16)   27712       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv_lstm2d_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 16)          0           ['batch_normalization_1[0][0]']  \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " cloud (Conv2D)                 (None, 32, 32, 1)    17          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " convective (Conv2D)            (None, 32, 32, 1)    17          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " fog (Conv2D)                   (None, 32, 32, 1)    17          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " moisture (Conv2D)              (None, 32, 32, 1)    17          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " temp_trend (Dense)             (None, 1)            17          ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " thermo_contrast (Conv2D)       (None, 32, 32, 1)    17          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,758\n",
      "Trainable params: 70,662\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Definition & Compilation\n",
    "\n",
    "inp = layers.Input((SEQ_LEN,PATCH_SIZE,PATCH_SIZE,5))\n",
    "x = layers.ConvLSTM2D(32,(3,3),padding='same',return_sequences=True,activation='relu')(inp)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(16,(3,3),padding='same',return_sequences=False,activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "heads = {\n",
    "  'cloud'          : layers.Conv2D(1,(1,1),activation='sigmoid',   name='cloud')(x),\n",
    "  'convective'     : layers.Conv2D(1,(1,1),activation='sigmoid',   name='convective')(x),\n",
    "  'fog'            : layers.Conv2D(1,(1,1),activation='sigmoid',   name='fog')(x),\n",
    "  'moisture'       : layers.Conv2D(1,(1,1),activation='linear',    name='moisture')(x),\n",
    "  'thermo_contrast': layers.Conv2D(1,(1,1),activation='linear',    name='thermo_contrast')(x),\n",
    "}\n",
    "temp_avg = layers.GlobalAveragePooling2D()(x)\n",
    "heads['temp_trend'] = layers.Dense(1,activation='linear',name='temp_trend')(temp_avg)\n",
    "\n",
    "model = Model(inputs=inp, outputs=heads, name='fast_multitask_nowcast')\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss={\n",
    "    'cloud':'binary_crossentropy','convective':'binary_crossentropy','fog':'binary_crossentropy',\n",
    "    'moisture':'mse','thermo_contrast':'mse','temp_trend':'mse'\n",
    "  },\n",
    "  loss_weights={'cloud':1,'convective':1,'fog':1,'moisture':0.5,'thermo_contrast':0.5,'temp_trend':0.1},\n",
    "  metrics={'cloud':'accuracy','convective':'accuracy','fog':'accuracy'}\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a649438",
   "metadata": {},
   "source": [
    "Training with Progress Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5b â€” Resume training from the mostâ€‘recent `.h5` checkpoint\n",
    "This cell will  \n",
    "1. look in the existing `checkpoints/` folder for the latest `model_epoch_XX.h5`,  \n",
    "2. load the model (weights **and** optimizer state), then  \n",
    "3. call `model.fit` so training continues right after that epoch.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„  Resuming from checkpoints\\model_epoch_11.h5  (completed epoch 11)\n",
      "Epoch 12/20\n"
     ]
    }
   ],
   "source": [
    "# Cell 5b â€” Resume training from latest .h5 checkpoint\n",
    "import os, re, glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1ï¸âƒ£  Locate the newest .h5 checkpoint\n",
    "# ------------------------------------------------------------------\n",
    "CHECKPOINT_DIR = \"checkpoints\"           # existing relative folder\n",
    "ckpt_files = glob.glob(os.path.join(CHECKPOINT_DIR, \"*.h5\"))\n",
    "\n",
    "if not ckpt_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No .h5 checkpoints found in '{CHECKPOINT_DIR}'. \"\n",
    "        \"Run the training cell once to create them.\"\n",
    "    )\n",
    "\n",
    "def _epoch_num(fname):\n",
    "    m = re.search(r\"(\\d+)\", os.path.basename(fname))\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "ckpt_files.sort(key=_epoch_num)\n",
    "latest_ckpt   = ckpt_files[-1]\n",
    "initial_epoch = _epoch_num(latest_ckpt)\n",
    "\n",
    "print(f\"ðŸ”„  Resuming from {latest_ckpt}  (completed epoch {initial_epoch})\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2ï¸âƒ£  Load model (weights + optimizer + compile config)\n",
    "# ------------------------------------------------------------------\n",
    "model = tf.keras.models.load_model(latest_ckpt)\n",
    "\n",
    "# If the compile information wasn't saved (rare older TF versions),\n",
    "# reâ€‘compile exactly as before:\n",
    "# model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3ï¸âƒ£  Continue training\n",
    "# ------------------------------------------------------------------\n",
    "resume_ckpt = ModelCheckpoint(\n",
    "    filepath=os.path.join(CHECKPOINT_DIR, \"model_epoch_{epoch:02d}.h5\"),\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_resume = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,            # same total target as before\n",
    "    initial_epoch=initial_epoch,\n",
    "    callbacks=[resume_ckpt],\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=val_steps,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a6144",
   "metadata": {},
   "source": [
    "Evaluation & Metrics Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 1345it [18:55:11, 58.96s/it]"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” Evaluation (fast, streamed) with total count in tqdm -----------\n",
    "import tensorflow as tf, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ 1. Load the checkpoint you want to evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SAVED_MODEL = r\"C:\\college\\CV\\COSMOS\\checkpoints\\model_epoch_11.h5\"\n",
    "model = tf.keras.models.load_model(SAVED_MODEL, compile=False)\n",
    "\n",
    "# â”€â”€ 2. Define tasks and initialise streaming accumulators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "seg_keys   = [\"cloud\", \"convective\", \"fog\"]\n",
    "reg_keys   = [\"moisture\", \"thermo_contrast\", \"temp_trend\"]\n",
    "conf       = {k: np.zeros((2,2), dtype=np.int64) for k in seg_keys}\n",
    "reg_sum_abs= {k: 0.0 for k in reg_keys}\n",
    "reg_sum_sq = {k: 0.0 for k in reg_keys}\n",
    "reg_n      = {k: 0    for k in reg_keys}\n",
    "\n",
    "# â”€â”€ 2.5 Compute total batches for tqdm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "card = tf.data.experimental.cardinality(val_ds)\n",
    "try:\n",
    "    total_batches = int(card.numpy()) if card.numpy() >= 0 else None\n",
    "except:\n",
    "    total_batches = None\n",
    "\n",
    "# â”€â”€ 3. Stream through the validation set once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for Xb, yb in tqdm(val_ds, desc=\"Evaluation\", total=total_batches):\n",
    "    preds = model(Xb, training=False)\n",
    "\n",
    "    # --- segmentation heads --------------------------------------------------\n",
    "    for k in seg_keys:\n",
    "        y_true = tf.reshape(yb[k],   (-1,)).numpy().astype(np.uint8)\n",
    "        y_pred = (tf.reshape(preds[k], (-1,)) > 0.5).numpy().astype(np.uint8)\n",
    "        conf[k][0,0] += np.sum((y_true==0)&(y_pred==0))\n",
    "        conf[k][0,1] += np.sum((y_true==0)&(y_pred==1))\n",
    "        conf[k][1,0] += np.sum((y_true==1)&(y_pred==0))\n",
    "        conf[k][1,1] += np.sum((y_true==1)&(y_pred==1))\n",
    "\n",
    "    # --- regression heads ----------------------------------------------------\n",
    "    for k in reg_keys:\n",
    "        y_true = tf.reshape(yb[k],   (-1,)).numpy()\n",
    "        y_pred = tf.reshape(preds[k], (-1,)).numpy()\n",
    "        diff   = y_pred - y_true\n",
    "        reg_sum_abs[k] += np.abs(diff).sum()\n",
    "        reg_sum_sq[k]  += np.square(diff).sum()\n",
    "        reg_n[k]       += diff.size\n",
    "\n",
    "# â”€â”€ 4. Compute final metrics -------------------------------------------------\n",
    "seg_rows = []\n",
    "for k, cm in conf.items():\n",
    "    TN,FP,FN,TP = cm.ravel()\n",
    "    acc  = (TP+TN)/(TP+TN+FP+FN)\n",
    "    prec = TP/(TP+FP) if TP+FP else 0\n",
    "    rec  = TP/(TP+FN) if TP+FN else 0\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec else 0\n",
    "    seg_rows.append(dict(Task=k, Acc=acc, Prec=prec, Rec=rec, F1=f1,\n",
    "                         TN=TN, FP=FP, FN=FN, TP=TP))\n",
    "df_seg = pd.DataFrame(seg_rows).set_index(\"Task\")\n",
    "\n",
    "reg_rows = []\n",
    "for k in reg_keys:\n",
    "    n   = reg_n[k]\n",
    "    mse = reg_sum_sq[k]/n\n",
    "    mae = reg_sum_abs[k]/n\n",
    "    reg_rows.append(dict(Task=k, MSE=mse, MAE=mae))\n",
    "df_reg = pd.DataFrame(reg_rows).set_index(\"Task\")\n",
    "\n",
    "# â”€â”€ 5. Display results -------------------------------------------------------\n",
    "print(\"### Segmentation\"); display(df_seg)\n",
    "print(\"### Regression\");   display(df_reg)\n",
    "\n",
    "# â”€â”€ 6. Confusion-matrix heat-maps --------------------------------------------\n",
    "for k, cm in conf.items():\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title(f\"{k.capitalize()} â€“ Confusion matrix\")\n",
    "    plt.imshow(cm, cmap=\"Blues\"); plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "    for (i,j),v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.colorbar(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
